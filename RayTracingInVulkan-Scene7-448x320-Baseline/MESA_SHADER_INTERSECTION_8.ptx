.version 2.0
.target sm_10, map_f64_to_f32

// shader: MESA_SHADER_INTERSECTION
// inputs: 0
// outputs: 0
// uniforms: 0
// shared: 0
// decl_function main (0 params)
.entry MESA_SHADER_INTERSECTION_func8_main () {
	.reg .b64 %Sphere;
	rt_alloc_mem %Sphere, 64, 8192; // decl_var ray_hit_attrib INTERP_MODE_NONE vec4 Sphere


	.reg .u64 %temp_u64;
	.reg .u32 %temp_u32;
	.reg .f32 %temp_f32;
	.reg .pred %temp_pred;
	.reg .f32 %const1_f32;
	mov.f32 %const1_f32, 0F3f800000;

	.reg .f32 %const0_f32;
	mov.f32 %const0_f32, 0F00000000;

	.reg .u32 %const0_u32;
	mov.u32 %const0_u32, 0;

	.reg .u16 %const1_u16;
	mov.u16 %const1_u16, 1;

	// start_block block_0:
	// preds: 
	.reg .f32 %ssa_0;
	mov.f32 %ssa_0, 0F00000000; // vec1 32 ssa_0 = load_const (0x00000000 /* 0.000000 */)
	.reg .b32 %ssa_0_bits;
	mov.f32 %ssa_0_bits, 0F00000000;

	.reg .u32 %ssa_1;
	load_ray_instance_custom_index %ssa_1;	// vec1 32 ssa_1 = intrinsic load_ray_instance_custom_index () ()

	.reg .b64 %ssa_2;
	load_vulkan_descriptor %ssa_2, 0, 11, 7; // vec4 32 ssa_2 = intrinsic vulkan_resource_index (%ssa_0) (0, 11, 7) /* desc_set=0 */ /* binding=11 */ /* desc_type=SSBO */

	.reg .b64 %ssa_3;
	mov.b64 %ssa_3, %ssa_2; // vec4 32 ssa_3 = intrinsic load_vulkan_descriptor (%ssa_2) (7) /* desc_type=SSBO */

	.reg .b64 %ssa_4;
	mov.b64 %ssa_4, %ssa_3; // vec4 32 ssa_4 = deref_cast (SphereArray *)ssa_3 (ssbo SphereArray)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_5;
	add.u64 %ssa_5, %ssa_4, 0; // vec4 32 ssa_5 = deref_struct &ssa_4->field0 (ssbo vec4[]) /* &((SphereArray *)ssa_3)->field0 */

	.reg .b64 %ssa_6;
	.reg .u32 %ssa_6_array_index_32;
	.reg .u64 %ssa_6_array_index_64;
	mov.u32 %ssa_6_array_index_32, %ssa_1;
	mul.wide.u32 %ssa_6_array_index_64, %ssa_6_array_index_32, 16;
	add.u64 %ssa_6, %ssa_5, %ssa_6_array_index_64; // vec4 32 ssa_6 = deref_array &(*ssa_5)[ssa_1] (ssbo vec4) /* &((SphereArray *)ssa_3)->field0[ssa_1] */

	.reg .f32 %ssa_7_0;
	.reg .f32 %ssa_7_1;
	.reg .f32 %ssa_7_2;
	.reg .f32 %ssa_7_3;
	ld.global.f32 %ssa_7_0, [%ssa_6 + 0];
	ld.global.f32 %ssa_7_1, [%ssa_6 + 4];
	ld.global.f32 %ssa_7_2, [%ssa_6 + 8];
	ld.global.f32 %ssa_7_3, [%ssa_6 + 12];
// vec4 32 ssa_7 = intrinsic load_deref (%ssa_6) (16) /* access=16 */


	.reg .f32 %ssa_8_0;
	.reg .f32 %ssa_8_1;
	.reg .f32 %ssa_8_2;
	.reg .f32 %ssa_8_3;
	.reg .b64 %ssa_8_address;
	load_ray_world_origin %ssa_8_address; // vec3 32 ssa_8 = intrinsic load_ray_world_origin () ()
	ld.global.f32 %ssa_8_0, [%ssa_8_address + 0];
	ld.global.f32 %ssa_8_1, [%ssa_8_address + 4];
	ld.global.f32 %ssa_8_2, [%ssa_8_address + 8];
	ld.global.f32 %ssa_8_3, [%ssa_8_address + 12];

	.reg .f32 %ssa_9_0;
	.reg .f32 %ssa_9_1;
	.reg .f32 %ssa_9_2;
	.reg .f32 %ssa_9_3;
	.reg .b64 %ssa_9_address;
	load_ray_world_direction %ssa_9_address; // vec3 32 ssa_9 = intrinsic load_ray_world_direction () ()
	ld.global.f32 %ssa_9_0, [%ssa_9_address + 0];
	ld.global.f32 %ssa_9_1, [%ssa_9_address + 4];
	ld.global.f32 %ssa_9_2, [%ssa_9_address + 8];
	ld.global.f32 %ssa_9_3, [%ssa_9_address + 12];

	.reg .f32 %ssa_10;
	load_ray_t_min %ssa_10;	// vec1 32 ssa_10 = intrinsic load_ray_t_min () ()

	.reg .f32 %ssa_11;
	load_ray_t_max %ssa_11;	// vec1 32 ssa_11 = intrinsic load_ray_t_max () ()

	.reg .f32 %ssa_12;
	neg.f32 %ssa_12, %ssa_7_0; // vec1 32 ssa_12 = fneg ssa_7.x

	.reg .f32 %ssa_13;
	add.f32 %ssa_13, %ssa_8_0, %ssa_12; // vec1 32 ssa_13 = fadd ssa_8.x, ssa_12

	.reg .f32 %ssa_14;
	neg.f32 %ssa_14, %ssa_7_2; // vec1 32 ssa_14 = fneg ssa_7.z

	.reg .f32 %ssa_15;
	add.f32 %ssa_15, %ssa_8_2, %ssa_14; // vec1 32 ssa_15 = fadd ssa_8.z, ssa_14

	.reg .f32 %ssa_16;
	mul.f32 %ssa_16, %ssa_9_2, %ssa_9_2; // vec1 32 ssa_16 = fmul ssa_9.z, ssa_9.z

	.reg .f32 %ssa_17;
	mul.f32 %ssa_17, %ssa_9_0, %ssa_9_0; // vec1 32 ssa_17 = fmul ssa_9.x, ssa_9.x

	.reg .f32 %ssa_18;
	add.f32 %ssa_18, %ssa_16, %ssa_17;	// vec1 32 ssa_18 = fadd ssa_16, ssa_17

	.reg .f32 %ssa_19;
	mul.f32 %ssa_19, %ssa_15, %ssa_9_2; // vec1 32 ssa_19 = fmul ssa_15, ssa_9.z

	.reg .f32 %ssa_20;
	mul.f32 %ssa_20, %ssa_13, %ssa_9_0; // vec1 32 ssa_20 = fmul ssa_13, ssa_9.x

	.reg .f32 %ssa_21;
	add.f32 %ssa_21, %ssa_19, %ssa_20;	// vec1 32 ssa_21 = fadd ssa_19, ssa_20

	.reg .f32 %ssa_22;
	mul.f32 %ssa_22, %ssa_15, %ssa_15;	// vec1 32 ssa_22 = fmul ssa_15, ssa_15

	.reg .f32 %ssa_23;
	mul.f32 %ssa_23, %ssa_13, %ssa_13;	// vec1 32 ssa_23 = fmul ssa_13, ssa_13

	.reg .f32 %ssa_24;
	add.f32 %ssa_24, %ssa_22, %ssa_23;	// vec1 32 ssa_24 = fadd ssa_22, ssa_23

	.reg .f32 %ssa_25;
	mul.f32 %ssa_25, %ssa_7_3, %ssa_7_3; // vec1 32 ssa_25 = fmul ssa_7.w, ssa_7.w

	.reg .f32 %ssa_26;
	neg.f32 %ssa_26, %ssa_25;	// vec1 32 ssa_26 = fneg ssa_25

	.reg .f32 %ssa_27;
	add.f32 %ssa_27, %ssa_24, %ssa_26;	// vec1 32 ssa_27 = fadd ssa_24, ssa_26

	.reg .f32 %ssa_28;
	mul.f32 %ssa_28, %ssa_21, %ssa_21;	// vec1 32 ssa_28 = fmul ssa_21, ssa_21

	.reg .f32 %ssa_29;
	mul.f32 %ssa_29, %ssa_18, %ssa_27;	// vec1 32 ssa_29 = fmul ssa_18, ssa_27

	.reg .f32 %ssa_30;
	neg.f32 %ssa_30, %ssa_29;	// vec1 32 ssa_30 = fneg ssa_29

	.reg .f32 %ssa_31;
	add.f32 %ssa_31, %ssa_28, %ssa_30;	// vec1 32 ssa_31 = fadd ssa_28, ssa_30

	.reg .pred %ssa_32;
	setp.ge.f32 %ssa_32, %ssa_31, %ssa_0;	// vec1 1 ssa_32 = fge! ssa_31, ssa_0

	// succs: block_1 block_5 
	// end_block block_0:
	//if
	@!%ssa_32 bra else_55;
	
		// start_block block_1:
		// preds: block_0 
		.reg .f32 %ssa_33;
		neg.f32 %ssa_33, %ssa_21;	// vec1 32 ssa_33 = fneg ssa_21

		.reg .f32 %ssa_34;
		sqrt.approx.f32 %ssa_34, %ssa_31;	// vec1 32 ssa_34 = fsqrt ssa_31

		.reg .f32 %ssa_35;
		neg.f32 %ssa_35, %ssa_34;	// vec1 32 ssa_35 = fneg ssa_34

		.reg .f32 %ssa_36;
		add.f32 %ssa_36, %ssa_33, %ssa_35;	// vec1 32 ssa_36 = fadd ssa_33, ssa_35

		.reg .f32 %ssa_37;
		rcp.approx.f32 %ssa_37, %ssa_18;	// vec1 32 ssa_37 = frcp ssa_18

		.reg .f32 %ssa_38;
		mul.f32 %ssa_38, %ssa_36, %ssa_37;	// vec1 32 ssa_38 = fmul ssa_36, ssa_37

		.reg .f32 %ssa_39;
		add.f32 %ssa_39, %ssa_33, %ssa_34;	// vec1 32 ssa_39 = fadd ssa_33, ssa_34

		.reg .f32 %ssa_40;
		mul.f32 %ssa_40, %ssa_39, %ssa_37;	// vec1 32 ssa_40 = fmul ssa_39, ssa_37

		.reg .f32 %ssa_41;
		mul.f32 %ssa_41, %ssa_9_1, %ssa_38; // vec1 32 ssa_41 = fmul ssa_9.y, ssa_38

		.reg .f32 %ssa_42;
		add.f32 %ssa_42, %ssa_8_1, %ssa_41; // vec1 32 ssa_42 = fadd ssa_8.y, ssa_41

		.reg .f32 %ssa_43;
		mul.f32 %ssa_43, %ssa_9_1, %ssa_40; // vec1 32 ssa_43 = fmul ssa_9.y, ssa_40

		.reg .f32 %ssa_44;
		add.f32 %ssa_44, %ssa_8_1, %ssa_43; // vec1 32 ssa_44 = fadd ssa_8.y, ssa_43

		.reg .f32 %ssa_45;
		add.f32 %ssa_45, %ssa_7_1, %ssa_7_3; // vec1 32 ssa_45 = fadd ssa_7.y, ssa_7.w

		.reg .pred %ssa_46;
		setp.ge.f32 %ssa_46, %ssa_45, %ssa_42;	// vec1 1 ssa_46 = fge! ssa_45, ssa_42

		.reg .f32 %ssa_47;
		neg.f32 %ssa_47, %ssa_7_3; // vec1 32 ssa_47 = fneg ssa_7.w

		.reg .f32 %ssa_48;
		add.f32 %ssa_48, %ssa_7_1, %ssa_47; // vec1 32 ssa_48 = fadd ssa_7.y, ssa_47

		.reg .pred %ssa_49;
		setp.ge.f32 %ssa_49, %ssa_42, %ssa_48;	// vec1 1 ssa_49 = fge! ssa_42, ssa_48

		.reg .pred %ssa_50;
		and.pred %ssa_50, %ssa_46, %ssa_49;	// vec1 1 ssa_50 = iand ssa_46, ssa_49

		.reg .pred %ssa_51;
		setp.ge.f32 %ssa_51, %ssa_45, %ssa_44;	// vec1 1 ssa_51 = fge! ssa_45, ssa_44

		.reg .pred %ssa_52;
		setp.ge.f32 %ssa_52, %ssa_44, %ssa_48;	// vec1 1 ssa_52 = fge! ssa_44, ssa_48

		.reg .pred %ssa_53;
		and.pred %ssa_53, %ssa_51, %ssa_52;	// vec1 1 ssa_53 = iand ssa_51, ssa_52

		.reg .pred %ssa_54;
		setp.ge.f32 %ssa_54, %ssa_38, %ssa_10;	// vec1 1 ssa_54 = fge! ssa_38, ssa_10

		.reg .pred %ssa_55;
		setp.lt.f32 %ssa_55, %ssa_38, %ssa_11;	// vec1 1 ssa_55 = flt! ssa_38, ssa_11

		.reg .pred %ssa_56;
		and.pred %ssa_56, %ssa_54, %ssa_55;	// vec1 1 ssa_56 = iand ssa_54, ssa_55

		.reg .pred %ssa_57;
		and.pred %ssa_57, %ssa_56, %ssa_50;	// vec1 1 ssa_57 = iand ssa_56, ssa_50

		.reg .pred %ssa_58;
		setp.ge.f32 %ssa_58, %ssa_40, %ssa_10;	// vec1 1 ssa_58 = fge! ssa_40, ssa_10

		.reg .pred %ssa_59;
		setp.lt.f32 %ssa_59, %ssa_40, %ssa_11;	// vec1 1 ssa_59 = flt! ssa_40, ssa_11

		.reg .pred %ssa_60;
		and.pred %ssa_60, %ssa_58, %ssa_59;	// vec1 1 ssa_60 = iand ssa_58, ssa_59

		.reg .pred %ssa_61;
		and.pred %ssa_61, %ssa_60, %ssa_53;	// vec1 1 ssa_61 = iand ssa_60, ssa_53

		.reg .pred %ssa_62;
		or.pred %ssa_62, %ssa_57, %ssa_61;	// vec1 1 ssa_62 = ior ssa_57, ssa_61

		// succs: block_2 block_3 
		// end_block block_1:
		//if
		@!%ssa_62 bra else_56;
		
			// start_block block_2:
			// preds: block_1 
			.reg .b64 %ssa_63;
	mov.b64 %ssa_63, %Sphere; // vec1 32 ssa_63 = deref_var &Sphere (ray_hit_attrib vec4) 

	st.global.f32 [%ssa_63 + 0], %ssa_7_0;
	st.global.f32 [%ssa_63 + 4], %ssa_7_1;
	st.global.f32 [%ssa_63 + 8], %ssa_7_2;
	st.global.f32 [%ssa_63 + 12], %ssa_7_3;
// intrinsic store_deref (%ssa_63, %ssa_7) (15, 0) /* wrmask=xyzw */ /* access=0 */


			.reg  .f32 %ssa_64;
			selp.f32 %ssa_64, %ssa_38, %ssa_40, %ssa_56; // vec1 32 ssa_64 = bcsel ssa_56, ssa_38, ssa_40

			.reg .pred %ssa_65;
			report_ray_intersection.pred %ssa_65, %ssa_64, %ssa_0;	// vec1 1 ssa_65 = intrinsic report_ray_intersection (%ssa_64, %ssa_0) ()

			// succs: block_4 
			// end_block block_2:
			bra end_if_56;
		
		else_56: 
			// start_block block_3:
			// preds: block_1 
			// succs: block_4 
			// end_block block_3:
		end_if_56:
		// start_block block_4:
		// preds: block_2 block_3 
		// succs: block_6 
		// end_block block_4:
		bra end_if_55;
	
	else_55: 
		// start_block block_5:
		// preds: block_0 
		// succs: block_6 
		// end_block block_5:
	end_if_55:
	// start_block block_6:
	// preds: block_4 block_5 
	// succs: block_7 
	// end_block block_6:
	// block block_7:
	shader_exit:
	ret ;
}
