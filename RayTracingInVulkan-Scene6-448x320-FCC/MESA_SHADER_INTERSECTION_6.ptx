.version 2.0
.target sm_10, map_f64_to_f32

// shader: MESA_SHADER_INTERSECTION
// inputs: 0
// outputs: 0
// uniforms: 0
// shared: 0
// decl_function main (0 params)
.entry MESA_SHADER_INTERSECTION_func6_main () {


	.reg .u64 %temp_u64;
	.reg .u32 %temp_u32;
	.reg .f32 %temp_f32;
	.reg .pred %temp_pred;
	.reg .f32 %const1_f32;
	mov.f32 %const1_f32, 0F3f800000;

	.reg .f32 %const0_f32;
	mov.f32 %const0_f32, 0F00000000;

	.reg .u32 %const0_u32;
	mov.u32 %const0_u32, 0;

	.reg .u16 %const1_u16;
	mov.u16 %const1_u16, 1;

	// start_block block_0:
	// preds: 
	.reg .f32 %ssa_0;
	mov.f32 %ssa_0, 0F00000000; // vec1 32 ssa_0 = load_const (0x00000000 /* 0.000000 */)
	.reg .b32 %ssa_0_bits;
	mov.f32 %ssa_0_bits, 0F00000000;

	.reg .u32 %ssa_1;
	load_ray_instance_custom_index %ssa_1;	// vec1 32 ssa_1 = intrinsic load_ray_instance_custom_index () ()

	.reg .b64 %ssa_2;
	load_vulkan_descriptor %ssa_2, 0, 10, 7; // vec4 32 ssa_2 = intrinsic vulkan_resource_index (%ssa_0) (0, 10, 7) /* desc_set=0 */ /* binding=10 */ /* desc_type=SSBO */

	.reg .b64 %ssa_3;
	mov.b64 %ssa_3, %ssa_2; // vec4 32 ssa_3 = intrinsic load_vulkan_descriptor (%ssa_2) (7) /* desc_type=SSBO */

	.reg .b64 %ssa_4;
	mov.b64 %ssa_4, %ssa_3; // vec4 32 ssa_4 = deref_cast (CubeArray *)ssa_3 (ssbo CubeArray)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_5;
	add.u64 %ssa_5, %ssa_4, 0; // vec4 32 ssa_5 = deref_struct &ssa_4->field0 (ssbo vec4[]) /* &((CubeArray *)ssa_3)->field0 */

	.reg .b64 %ssa_6;
	.reg .u32 %ssa_6_array_index_32;
	.reg .u64 %ssa_6_array_index_64;
	mov.u32 %ssa_6_array_index_32, %ssa_1;
	mul.wide.u32 %ssa_6_array_index_64, %ssa_6_array_index_32, 16;
	add.u64 %ssa_6, %ssa_5, %ssa_6_array_index_64; // vec4 32 ssa_6 = deref_array &(*ssa_5)[ssa_1] (ssbo vec4) /* &((CubeArray *)ssa_3)->field0[ssa_1] */

	.reg .f32 %ssa_7_0;
	.reg .f32 %ssa_7_1;
	.reg .f32 %ssa_7_2;
	.reg .f32 %ssa_7_3;
	ld.global.f32 %ssa_7_0, [%ssa_6 + 0];
	ld.global.f32 %ssa_7_1, [%ssa_6 + 4];
	ld.global.f32 %ssa_7_2, [%ssa_6 + 8];
	ld.global.f32 %ssa_7_3, [%ssa_6 + 12];
// vec4 32 ssa_7 = intrinsic load_deref (%ssa_6) (16) /* access=16 */


	.reg .f32 %ssa_8_0;
	.reg .f32 %ssa_8_1;
	.reg .f32 %ssa_8_2;
	.reg .f32 %ssa_8_3;
	.reg .b64 %ssa_8_address;
	load_ray_world_origin %ssa_8_address; // vec3 32 ssa_8 = intrinsic load_ray_world_origin () ()
	ld.global.f32 %ssa_8_0, [%ssa_8_address + 0];
	ld.global.f32 %ssa_8_1, [%ssa_8_address + 4];
	ld.global.f32 %ssa_8_2, [%ssa_8_address + 8];
	ld.global.f32 %ssa_8_3, [%ssa_8_address + 12];

	.reg .f32 %ssa_9_0;
	.reg .f32 %ssa_9_1;
	.reg .f32 %ssa_9_2;
	.reg .f32 %ssa_9_3;
	.reg .b64 %ssa_9_address;
	load_ray_world_direction %ssa_9_address; // vec3 32 ssa_9 = intrinsic load_ray_world_direction () ()
	ld.global.f32 %ssa_9_0, [%ssa_9_address + 0];
	ld.global.f32 %ssa_9_1, [%ssa_9_address + 4];
	ld.global.f32 %ssa_9_2, [%ssa_9_address + 8];
	ld.global.f32 %ssa_9_3, [%ssa_9_address + 12];

	.reg .f32 %ssa_10;
	neg.f32 %ssa_10, %ssa_7_3; // vec1 32 ssa_10 = fneg ssa_7.w

	.reg .f32 %ssa_11;
	add.f32 %ssa_11, %ssa_7_0, %ssa_10; // vec1 32 ssa_11 = fadd ssa_7.x, ssa_10

	.reg .f32 %ssa_12;
	add.f32 %ssa_12, %ssa_7_1, %ssa_10; // vec1 32 ssa_12 = fadd ssa_7.y, ssa_10

	.reg .f32 %ssa_13;
	add.f32 %ssa_13, %ssa_7_2, %ssa_10; // vec1 32 ssa_13 = fadd ssa_7.z, ssa_10

	.reg .f32 %ssa_14;
	add.f32 %ssa_14, %ssa_7_0, %ssa_7_3; // vec1 32 ssa_14 = fadd ssa_7.x, ssa_7.w

	.reg .f32 %ssa_15;
	add.f32 %ssa_15, %ssa_7_1, %ssa_7_3; // vec1 32 ssa_15 = fadd ssa_7.y, ssa_7.w

	.reg .f32 %ssa_16;
	add.f32 %ssa_16, %ssa_7_2, %ssa_7_3; // vec1 32 ssa_16 = fadd ssa_7.z, ssa_7.w

	.reg .f32 %ssa_17;
	mov.f32 %ssa_17, 0Fbf800000; // vec1 32 ssa_17 = load_const (0xbf800000 /* -1.000000 */)
	.reg .b32 %ssa_17_bits;
	mov.f32 %ssa_17_bits, 0Fbf800000;

	.reg .f32 %ssa_18;
	rcp.approx.f32 %ssa_18, %ssa_9_0; // vec1 32 ssa_18 = frcp ssa_9.x

	.reg .f32 %ssa_19;
	rcp.approx.f32 %ssa_19, %ssa_9_1; // vec1 32 ssa_19 = frcp ssa_9.y

	.reg .f32 %ssa_20;
	rcp.approx.f32 %ssa_20, %ssa_9_2; // vec1 32 ssa_20 = frcp ssa_9.z

	.reg .f32 %ssa_21;
	neg.f32 %ssa_21, %ssa_8_0; // vec1 32 ssa_21 = fneg ssa_8.x

	.reg .f32 %ssa_22;
	add.f32 %ssa_22, %ssa_11, %ssa_21;	// vec1 32 ssa_22 = fadd ssa_11, ssa_21

	.reg .f32 %ssa_23;
	neg.f32 %ssa_23, %ssa_8_1; // vec1 32 ssa_23 = fneg ssa_8.y

	.reg .f32 %ssa_24;
	add.f32 %ssa_24, %ssa_12, %ssa_23;	// vec1 32 ssa_24 = fadd ssa_12, ssa_23

	.reg .f32 %ssa_25;
	neg.f32 %ssa_25, %ssa_8_2; // vec1 32 ssa_25 = fneg ssa_8.z

	.reg .f32 %ssa_26;
	add.f32 %ssa_26, %ssa_13, %ssa_25;	// vec1 32 ssa_26 = fadd ssa_13, ssa_25

	.reg .f32 %ssa_27;
	mul.f32 %ssa_27, %ssa_18, %ssa_22;	// vec1 32 ssa_27 = fmul ssa_18, ssa_22

	.reg .f32 %ssa_28;
	mul.f32 %ssa_28, %ssa_19, %ssa_24;	// vec1 32 ssa_28 = fmul ssa_19, ssa_24

	.reg .f32 %ssa_29;
	mul.f32 %ssa_29, %ssa_20, %ssa_26;	// vec1 32 ssa_29 = fmul ssa_20, ssa_26

	.reg .f32 %ssa_30;
	add.f32 %ssa_30, %ssa_14, %ssa_21;	// vec1 32 ssa_30 = fadd ssa_14, ssa_21

	.reg .f32 %ssa_31;
	add.f32 %ssa_31, %ssa_15, %ssa_23;	// vec1 32 ssa_31 = fadd ssa_15, ssa_23

	.reg .f32 %ssa_32;
	add.f32 %ssa_32, %ssa_16, %ssa_25;	// vec1 32 ssa_32 = fadd ssa_16, ssa_25

	.reg .f32 %ssa_33;
	mul.f32 %ssa_33, %ssa_18, %ssa_30;	// vec1 32 ssa_33 = fmul ssa_18, ssa_30

	.reg .f32 %ssa_34;
	mul.f32 %ssa_34, %ssa_19, %ssa_31;	// vec1 32 ssa_34 = fmul ssa_19, ssa_31

	.reg .f32 %ssa_35;
	mul.f32 %ssa_35, %ssa_20, %ssa_32;	// vec1 32 ssa_35 = fmul ssa_20, ssa_32

	.reg .f32 %ssa_36;
	min.f32 %ssa_36, %ssa_33, %ssa_27;	// vec1 32 ssa_36 = fmin ssa_33, ssa_27

	.reg .f32 %ssa_37;
	min.f32 %ssa_37, %ssa_34, %ssa_28;	// vec1 32 ssa_37 = fmin ssa_34, ssa_28

	.reg .f32 %ssa_38;
	min.f32 %ssa_38, %ssa_35, %ssa_29;	// vec1 32 ssa_38 = fmin ssa_35, ssa_29

	.reg .f32 %ssa_39;
	max.f32 %ssa_39, %ssa_33, %ssa_27;	// vec1 32 ssa_39 = fmax ssa_33, ssa_27

	.reg .f32 %ssa_40;
	max.f32 %ssa_40, %ssa_34, %ssa_28;	// vec1 32 ssa_40 = fmax ssa_34, ssa_28

	.reg .f32 %ssa_41;
	max.f32 %ssa_41, %ssa_35, %ssa_29;	// vec1 32 ssa_41 = fmax ssa_35, ssa_29

	.reg .f32 %ssa_42;
	max.f32 %ssa_42, %ssa_37, %ssa_38;	// vec1 32 ssa_42 = fmax ssa_37, ssa_38

	.reg .f32 %ssa_43;
	max.f32 %ssa_43, %ssa_36, %ssa_42;	// vec1 32 ssa_43 = fmax ssa_36, ssa_42

	.reg .f32 %ssa_44;
	min.f32 %ssa_44, %ssa_40, %ssa_41;	// vec1 32 ssa_44 = fmin ssa_40, ssa_41

	.reg .f32 %ssa_45;
	min.f32 %ssa_45, %ssa_39, %ssa_44;	// vec1 32 ssa_45 = fmin ssa_39, ssa_44

	.reg .f32 %ssa_46;
	max.f32 %ssa_46, %ssa_43, %ssa_0;	// vec1 32 ssa_46 = fmax ssa_43, ssa_0

	.reg .pred %ssa_47;
	setp.lt.f32 %ssa_47, %ssa_46, %ssa_45;	// vec1 1 ssa_47 = flt! ssa_46, ssa_45

	.reg  .f32 %ssa_48;
	selp.f32 %ssa_48, %ssa_43, %ssa_17_bits, %ssa_47; // vec1 32 ssa_48 = bcsel ssa_47, ssa_43, ssa_17

	.reg .pred %ssa_49;
	report_ray_intersection.pred %ssa_49, %ssa_48, %ssa_0;	// vec1 1 ssa_49 = intrinsic report_ray_intersection (%ssa_48, %ssa_0) ()

	// succs: block_1 
	// end_block block_0:
	// block block_1:
	shader_exit:
	ret ;
}
