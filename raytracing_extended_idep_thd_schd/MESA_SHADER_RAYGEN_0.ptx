.version 2.0
.target sm_10, map_f64_to_f32

// shader: MESA_SHADER_RAYGEN
// inputs: 0
// outputs: 0
// uniforms: 0
// ubos: 1
// shared: 0
// decl_function main (0 params)
.entry MESA_SHADER_RAYGEN_func0_main () {
	.reg .u32 %launch_ID_0;
	.reg .u32 %launch_ID_1;
	.reg .u32 %launch_ID_2;
	load_ray_launch_id %launch_ID_0, %launch_ID_1, %launch_ID_2;
	
	.reg .u32 %launch_Size_0;
	.reg .u32 %launch_Size_1;
	.reg .u32 %launch_Size_2;
	load_ray_launch_size %launch_Size_0, %launch_Size_1, %launch_Size_2;
	
	
	.reg .pred %bigger_0;
	setp.ge.u32 %bigger_0, %launch_ID_0, %launch_Size_0;
	
	.reg .pred %bigger_1;
	setp.ge.u32 %bigger_1, %launch_ID_1, %launch_Size_1;
	
	.reg .pred %bigger_2;
	setp.ge.u32 %bigger_2, %launch_ID_2, %launch_Size_2;
	
	@%bigger_0 bra shader_exit;
	@%bigger_1 bra shader_exit;
	@%bigger_2 bra shader_exit;

		.reg  .b32 %ssa_351;

		.reg  .b32 %ssa_350;

		.reg  .b32 %ssa_349;

		.reg  .b32 %ssa_348;

		.reg  .b32 %ssa_347;

		.reg  .b32 %ssa_346;

		.reg  .b32 %ssa_345;

		.reg  .b32 %ssa_344;

		.reg  .b32 %ssa_343;

		.reg  .b32 %ssa_342;

		.reg  .b32 %ssa_341;

		.reg  .b32 %ssa_340;

			.reg  .b32 %ssa_339;

			.reg  .b32 %ssa_338;

			.reg  .b32 %ssa_337;

			.reg  .b32 %ssa_336;

			.reg  .b32 %ssa_335;

			.reg  .b32 %ssa_334;

			.reg  .b32 %ssa_333;

			.reg  .b32 %ssa_332;

			.reg  .b32 %ssa_331;

			.reg  .b32 %ssa_330;

			.reg  .b32 %ssa_329;

				.reg  .b32 %ssa_328;

				.reg  .b32 %ssa_327;

				.reg  .b32 %ssa_326;

				.reg  .b32 %ssa_325;

				.reg  .b32 %ssa_324;

				.reg  .b32 %ssa_323;

				.reg  .b32 %ssa_322;

		.reg  .f32 %ssa_119;

		.reg  .s32 %ssa_118;

		.reg  .b32 %ssa_117;

		.reg  .b32 %ssa_116;

		.reg  .b32 %ssa_115;

		.reg  .b32 %ssa_114;

		.reg  .b32 %ssa_113;

		.reg  .b32 %ssa_112;

		.reg  .b32 %ssa_111;

		.reg  .b32 %ssa_110;

		.reg  .b32 %ssa_109;

		.reg  .b32 %ssa_108;

		.reg  .b32 %ssa_107;

		.reg  .b32 %ssa_106;

	.reg .b64 %image;
	load_vulkan_descriptor %image, 0, 1; // decl_var uniform INTERP_MODE_NONE restrict r8g8b8a8_unorm image2D image (~0, 0, 1)
	.reg .b64 %hitValue;
	rt_alloc_mem %hitValue, 48, 8; // decl_var  INTERP_MODE_NONE Payload hitValue


	.reg .u64 %temp_u64;
	.reg .u32 %temp_u32;
	.reg .f32 %temp_f32;
	.reg .pred %temp_pred;
	.reg .f32 %const1_f32;
	mov.f32 %const1_f32, 0F3f800000;

	.reg .f32 %const0_f32;
	mov.f32 %const0_f32, 0F00000000;

	.reg .u32 %const0_u32;
	mov.u32 %const0_u32, 0;

	.reg .u16 %const1_u16;
	mov.u16 %const1_u16, 1;

	// start_block block_0:
	// preds: 
	.reg .f32 %ssa_0;
	mov.f32 %ssa_0, 0F000000ff; // vec1 32 ssa_0 = undefined
	.reg .b32 %ssa_0_bits;
	mov.f32 %ssa_0_bits, 0F000000ff;

	.reg .f32 %ssa_1;
	mov.f32 %ssa_1, 0F00000001; // vec1 32 ssa_1 = load_const (0x00000001 /* 0.000000 */)
	.reg .b32 %ssa_1_bits;
	mov.f32 %ssa_1_bits, 0F00000001;

	.reg .f32 %ssa_2;
	mov.f32 %ssa_2, 0F3dcccccd; // vec1 32 ssa_2 = load_const (0x3dcccccd /* 0.100000 */)
	.reg .b32 %ssa_2_bits;
	mov.f32 %ssa_2_bits, 0F3dcccccd;

	.reg .f32 %ssa_3;
	mov.f32 %ssa_3, 0F3f800000; // vec1 32 ssa_3 = load_const (0x3f800000 /* 1.000000 */)
	.reg .b32 %ssa_3_bits;
	mov.f32 %ssa_3_bits, 0F3f800000;

	.reg .f32 %ssa_4;
	mov.f32 %ssa_4, 0F00000002; // vec1 32 ssa_4 = load_const (0x00000002 /* 0.000000 */)
	.reg .b32 %ssa_4_bits;
	mov.f32 %ssa_4_bits, 0F00000002;

	.reg .f32 %ssa_5;
	mov.f32 %ssa_5, 0F00000000; // vec1 32 ssa_5 = load_const (0x00000000 /* 0.000000 */)
	.reg .b32 %ssa_5_bits;
	mov.f32 %ssa_5_bits, 0F00000000;

	.reg .f32 %ssa_6;
	mov.f32 %ssa_6, 0F40000000; // vec1 32 ssa_6 = load_const (0x40000000 /* 2.000000 */)
	.reg .b32 %ssa_6_bits;
	mov.f32 %ssa_6_bits, 0F40000000;

	.reg .f32 %ssa_7;
	mov.f32 %ssa_7, 0F000000ff; // vec1 32 ssa_7 = load_const (0x000000ff /* 0.000000 */)
	.reg .b32 %ssa_7_bits;
	mov.f32 %ssa_7_bits, 0F000000ff;

	.reg .f32 %ssa_8;
	mov.f32 %ssa_8, 0F3f000000; // vec1 32 ssa_8 = load_const (0x3f000000 /* 0.500000 */)
	.reg .b32 %ssa_8_bits;
	mov.f32 %ssa_8_bits, 0F3f000000;

	.reg .f32 %ssa_9;
	mov.f32 %ssa_9, 0F3f666666; // vec1 32 ssa_9 = load_const (0x3f666666 /* 0.900000 */)
	.reg .b32 %ssa_9_bits;
	mov.f32 %ssa_9_bits, 0F3f666666;

	.reg .f32 %ssa_10;
	mov.f32 %ssa_10, 0F00000065; // vec1 32 ssa_10 = load_const (0x00000065 /* 0.000000 */)
	.reg .b32 %ssa_10_bits;
	mov.f32 %ssa_10_bits, 0F00000065;

	.reg .f32 %ssa_11;
	mov.f32 %ssa_11, 0Fc1a00000; // vec1 32 ssa_11 = load_const (0xc1a00000 /* -20.000000 */)
	.reg .b32 %ssa_11_bits;
	mov.f32 %ssa_11_bits, 0Fc1a00000;

	.reg .f32 %ssa_12;
	mov.f32 %ssa_12, 0F3f7d70a4; // vec1 32 ssa_12 = load_const (0x3f7d70a4 /* 0.990000 */)
	.reg .b32 %ssa_12_bits;
	mov.f32 %ssa_12_bits, 0F3f7d70a4;

	.reg .f32 %ssa_13;
	mov.f32 %ssa_13, 0F3f733333; // vec1 32 ssa_13 = load_const (0x3f733333 /* 0.950000 */)
	.reg .b32 %ssa_13_bits;
	mov.f32 %ssa_13_bits, 0F3f733333;

	.reg .f32 %ssa_14;
	mov.f32 %ssa_14, 0F00000064; // vec1 32 ssa_14 = load_const (0x00000064 /* 0.000000 */)
	.reg .b32 %ssa_14_bits;
	mov.f32 %ssa_14_bits, 0F00000064;

	.reg .f32 %ssa_15;
	mov.f32 %ssa_15, 0F0000003c; // vec1 32 ssa_15 = load_const (0x0000003c /* 0.000000 */)
	.reg .b32 %ssa_15_bits;
	mov.f32 %ssa_15_bits, 0F0000003c;

	.reg .f32 %ssa_16;
	mov.f32 %ssa_16, 0F461c3c00; // vec1 32 ssa_16 = load_const (0x461c3c00 /* 9999.000000 */)
	.reg .b32 %ssa_16_bits;
	mov.f32 %ssa_16_bits, 0F461c3c00;

	.reg .f32 %ssa_17;
	mov.f32 %ssa_17, 0F461c4000; // vec1 32 ssa_17 = load_const (0x461c4000 /* 10000.000000 */)
	.reg .b32 %ssa_17_bits;
	mov.f32 %ssa_17_bits, 0F461c4000;

	.reg .f32 %ssa_18;
	mov.f32 %ssa_18, 0F3b03126f; // vec1 32 ssa_18 = load_const (0x3b03126f /* 0.002000 */)
	.reg .b32 %ssa_18_bits;
	mov.f32 %ssa_18_bits, 0F3b03126f;

	.reg .f32 %ssa_19;
	mov.f32 %ssa_19, 0F3a83126f; // vec1 32 ssa_19 = load_const (0x3a83126f /* 0.001000 */)
	.reg .b32 %ssa_19_bits;
	mov.f32 %ssa_19_bits, 0F3a83126f;

	.reg .u32 %ssa_20_0;
	.reg .u32 %ssa_20_1;
	.reg .u32 %ssa_20_2;
	.reg .u32 %ssa_20_3;
	load_ray_launch_id %ssa_20_0, %ssa_20_1, %ssa_20_2; // vec3 32 ssa_20 = intrinsic load_ray_launch_id () ()

	.reg .f32 %ssa_21;
	cvt.rn.f32.u32 %ssa_21, %ssa_20_0; // vec1 32 ssa_21 = u2f32 ssa_20.x

	.reg .f32 %ssa_22;
	cvt.rn.f32.u32 %ssa_22, %ssa_20_1; // vec1 32 ssa_22 = u2f32 ssa_20.y

	.reg .f32 %ssa_23;
	add.f32 %ssa_23, %ssa_21, %ssa_8;	// vec1 32 ssa_23 = fadd ssa_21, ssa_8

	.reg .f32 %ssa_24;
	add.f32 %ssa_24, %ssa_22, %ssa_8;	// vec1 32 ssa_24 = fadd ssa_22, ssa_8

	.reg .u32 %ssa_25_0;
	.reg .u32 %ssa_25_1;
	.reg .u32 %ssa_25_2;
	.reg .u32 %ssa_25_3;
	load_ray_launch_size %ssa_25_0, %ssa_25_1, %ssa_25_2; // vec3 32 ssa_25 = intrinsic load_ray_launch_size () ()

	.reg .f32 %ssa_26;
	cvt.rn.f32.u32 %ssa_26, %ssa_25_0; // vec1 32 ssa_26 = u2f32 ssa_25.x

	.reg .f32 %ssa_27;
	cvt.rn.f32.u32 %ssa_27, %ssa_25_1; // vec1 32 ssa_27 = u2f32 ssa_25.y

	.reg .f32 %ssa_28;
	rcp.approx.f32 %ssa_28, %ssa_26;	// vec1 32 ssa_28 = frcp ssa_26

	.reg .f32 %ssa_29;
	rcp.approx.f32 %ssa_29, %ssa_27;	// vec1 32 ssa_29 = frcp ssa_27

	.reg .f32 %ssa_30;
	mul.f32 %ssa_30, %ssa_23, %ssa_6;	// vec1 32 ssa_30 = fmul ssa_23, ssa_6

	.reg .f32 %ssa_31;
	mul.f32 %ssa_31, %ssa_30, %ssa_28;	// vec1 32 ssa_31 = fmul ssa_30, ssa_28

	.reg .f32 %ssa_32;
	mul.f32 %ssa_32, %ssa_24, %ssa_6;	// vec1 32 ssa_32 = fmul ssa_24, ssa_6

	.reg .f32 %ssa_33;
	mul.f32 %ssa_33, %ssa_32, %ssa_29;	// vec1 32 ssa_33 = fmul ssa_32, ssa_29

	.reg .f32 %ssa_34;
	mov.f32 %ssa_34, 0Fbf800000; // vec1 32 ssa_34 = load_const (0xbf800000 /* -1.000000 */)
	.reg .b32 %ssa_34_bits;
	mov.f32 %ssa_34_bits, 0Fbf800000;

	.reg .f32 %ssa_35;
	add.f32 %ssa_35, %ssa_31, %ssa_34;	// vec1 32 ssa_35 = fadd ssa_31, ssa_34

	.reg .f32 %ssa_36;
	add.f32 %ssa_36, %ssa_33, %ssa_34;	// vec1 32 ssa_36 = fadd ssa_33, ssa_34

	.reg .b64 %ssa_37;
	load_vulkan_descriptor %ssa_37, 0, 2, 6; // vec4 32 ssa_37 = intrinsic vulkan_resource_index (%ssa_5) (0, 2, 6) /* desc_set=0 */ /* binding=2 */ /* desc_type=UBO */

	.reg .b64 %ssa_38;
	mov.b64 %ssa_38, %ssa_37; // vec4 32 ssa_38 = intrinsic load_vulkan_descriptor (%ssa_37) (6) /* desc_type=UBO */

	.reg .b64 %ssa_39;
	mov.b64 %ssa_39, %ssa_38; // vec4 32 ssa_39 = deref_cast (CameraProperties *)ssa_38 (ubo CameraProperties)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_40;
	add.u64 %ssa_40, %ssa_39, 0; // vec4 32 ssa_40 = deref_struct &ssa_39->field0 (ubo mat4x16a0B) /* &((CameraProperties *)ssa_38)->field0 */

	.reg .b64 %ssa_41;
	add.u64 %ssa_41, %ssa_40, 0; // vec4 32 ssa_41 = deref_array &(*ssa_40)[0] (ubo vec4) /* &((CameraProperties *)ssa_38)->field0[0] */

	.reg .f32 %ssa_42_0;
	.reg .f32 %ssa_42_1;
	.reg .f32 %ssa_42_2;
	.reg .f32 %ssa_42_3;
	ld.global.f32 %ssa_42_0, [%ssa_41 + 0];
	ld.global.f32 %ssa_42_1, [%ssa_41 + 4];
	ld.global.f32 %ssa_42_2, [%ssa_41 + 8];
	ld.global.f32 %ssa_42_3, [%ssa_41 + 12];
// vec4 32 ssa_42 = intrinsic load_deref (%ssa_41) (0) /* access=0 */


	.reg .b64 %ssa_43;
	add.u64 %ssa_43, %ssa_40, 16; // vec4 32 ssa_43 = deref_array &(*ssa_40)[1] (ubo vec4) /* &((CameraProperties *)ssa_38)->field0[1] */

	.reg .f32 %ssa_44_0;
	.reg .f32 %ssa_44_1;
	.reg .f32 %ssa_44_2;
	.reg .f32 %ssa_44_3;
	ld.global.f32 %ssa_44_0, [%ssa_43 + 0];
	ld.global.f32 %ssa_44_1, [%ssa_43 + 4];
	ld.global.f32 %ssa_44_2, [%ssa_43 + 8];
	ld.global.f32 %ssa_44_3, [%ssa_43 + 12];
// vec4 32 ssa_44 = intrinsic load_deref (%ssa_43) (0) /* access=0 */


	.reg .b64 %ssa_45;
	add.u64 %ssa_45, %ssa_40, 32; // vec4 32 ssa_45 = deref_array &(*ssa_40)[2] (ubo vec4) /* &((CameraProperties *)ssa_38)->field0[2] */

	.reg .f32 %ssa_46_0;
	.reg .f32 %ssa_46_1;
	.reg .f32 %ssa_46_2;
	.reg .f32 %ssa_46_3;
	ld.global.f32 %ssa_46_0, [%ssa_45 + 0];
	ld.global.f32 %ssa_46_1, [%ssa_45 + 4];
	ld.global.f32 %ssa_46_2, [%ssa_45 + 8];
	ld.global.f32 %ssa_46_3, [%ssa_45 + 12];
// vec4 32 ssa_46 = intrinsic load_deref (%ssa_45) (0) /* access=0 */


	.reg .f32 %ssa_47;
	mov.f32 %ssa_47, 0F00000003; // vec1 32 ssa_47 = load_const (0x00000003 /* 0.000000 */)
	.reg .b32 %ssa_47_bits;
	mov.f32 %ssa_47_bits, 0F00000003;

	.reg .b64 %ssa_48;
	add.u64 %ssa_48, %ssa_40, 48; // vec4 32 ssa_48 = deref_array &(*ssa_40)[3] (ubo vec4) /* &((CameraProperties *)ssa_38)->field0[3] */

	.reg .f32 %ssa_49_0;
	.reg .f32 %ssa_49_1;
	.reg .f32 %ssa_49_2;
	.reg .f32 %ssa_49_3;
	ld.global.f32 %ssa_49_0, [%ssa_48 + 0];
	ld.global.f32 %ssa_49_1, [%ssa_48 + 4];
	ld.global.f32 %ssa_49_2, [%ssa_48 + 8];
	ld.global.f32 %ssa_49_3, [%ssa_48 + 12];
// vec4 32 ssa_49 = intrinsic load_deref (%ssa_48) (0) /* access=0 */


	.reg .b64 %ssa_50;
	add.u64 %ssa_50, %ssa_39, 64; // vec4 32 ssa_50 = deref_struct &ssa_39->field1 (ubo mat4x16a0B) /* &((CameraProperties *)ssa_38)->field1 */

	.reg .b64 %ssa_51;
	add.u64 %ssa_51, %ssa_50, 0; // vec4 32 ssa_51 = deref_array &(*ssa_50)[0] (ubo vec4) /* &((CameraProperties *)ssa_38)->field1[0] */

	.reg .f32 %ssa_52_0;
	.reg .f32 %ssa_52_1;
	.reg .f32 %ssa_52_2;
	.reg .f32 %ssa_52_3;
	ld.global.f32 %ssa_52_0, [%ssa_51 + 0];
	ld.global.f32 %ssa_52_1, [%ssa_51 + 4];
	ld.global.f32 %ssa_52_2, [%ssa_51 + 8];
	ld.global.f32 %ssa_52_3, [%ssa_51 + 12];
// vec4 32 ssa_52 = intrinsic load_deref (%ssa_51) (0) /* access=0 */


	.reg .b64 %ssa_53;
	add.u64 %ssa_53, %ssa_50, 16; // vec4 32 ssa_53 = deref_array &(*ssa_50)[1] (ubo vec4) /* &((CameraProperties *)ssa_38)->field1[1] */

	.reg .f32 %ssa_54_0;
	.reg .f32 %ssa_54_1;
	.reg .f32 %ssa_54_2;
	.reg .f32 %ssa_54_3;
	ld.global.f32 %ssa_54_0, [%ssa_53 + 0];
	ld.global.f32 %ssa_54_1, [%ssa_53 + 4];
	ld.global.f32 %ssa_54_2, [%ssa_53 + 8];
	ld.global.f32 %ssa_54_3, [%ssa_53 + 12];
// vec4 32 ssa_54 = intrinsic load_deref (%ssa_53) (0) /* access=0 */


	.reg .b64 %ssa_55;
	add.u64 %ssa_55, %ssa_50, 32; // vec4 32 ssa_55 = deref_array &(*ssa_50)[2] (ubo vec4) /* &((CameraProperties *)ssa_38)->field1[2] */

	.reg .f32 %ssa_56_0;
	.reg .f32 %ssa_56_1;
	.reg .f32 %ssa_56_2;
	.reg .f32 %ssa_56_3;
	ld.global.f32 %ssa_56_0, [%ssa_55 + 0];
	ld.global.f32 %ssa_56_1, [%ssa_55 + 4];
	ld.global.f32 %ssa_56_2, [%ssa_55 + 8];
	ld.global.f32 %ssa_56_3, [%ssa_55 + 12];
// vec4 32 ssa_56 = intrinsic load_deref (%ssa_55) (0) /* access=0 */


	.reg .b64 %ssa_57;
	add.u64 %ssa_57, %ssa_50, 48; // vec4 32 ssa_57 = deref_array &(*ssa_50)[3] (ubo vec4) /* &((CameraProperties *)ssa_38)->field1[3] */

	.reg .f32 %ssa_58_0;
	.reg .f32 %ssa_58_1;
	.reg .f32 %ssa_58_2;
	.reg .f32 %ssa_58_3;
	ld.global.f32 %ssa_58_0, [%ssa_57 + 0];
	ld.global.f32 %ssa_58_1, [%ssa_57 + 4];
	ld.global.f32 %ssa_58_2, [%ssa_57 + 8];
	ld.global.f32 %ssa_58_3, [%ssa_57 + 12];
// vec4 32 ssa_58 = intrinsic load_deref (%ssa_57) (0) /* access=0 */


	.reg .f32 %ssa_59;
	add.f32 %ssa_59, %ssa_58_0, %ssa_56_0; // vec1 32 ssa_59 = fadd ssa_58.x, ssa_56.x

	.reg .f32 %ssa_60;
	add.f32 %ssa_60, %ssa_58_1, %ssa_56_1; // vec1 32 ssa_60 = fadd ssa_58.y, ssa_56.y

	.reg .f32 %ssa_61;
	add.f32 %ssa_61, %ssa_58_2, %ssa_56_2; // vec1 32 ssa_61 = fadd ssa_58.z, ssa_56.z

	.reg .f32 %ssa_62;
	mul.f32 %ssa_62, %ssa_54_0, %ssa_36; // vec1 32 ssa_62 = fmul ssa_54.x, ssa_36

	.reg .f32 %ssa_63;
	mul.f32 %ssa_63, %ssa_54_1, %ssa_36; // vec1 32 ssa_63 = fmul ssa_54.y, ssa_36

	.reg .f32 %ssa_64;
	mul.f32 %ssa_64, %ssa_54_2, %ssa_36; // vec1 32 ssa_64 = fmul ssa_54.z, ssa_36

	.reg .f32 %ssa_65;
	add.f32 %ssa_65, %ssa_59, %ssa_62;	// vec1 32 ssa_65 = fadd ssa_59, ssa_62

	.reg .f32 %ssa_66;
	add.f32 %ssa_66, %ssa_60, %ssa_63;	// vec1 32 ssa_66 = fadd ssa_60, ssa_63

	.reg .f32 %ssa_67;
	add.f32 %ssa_67, %ssa_61, %ssa_64;	// vec1 32 ssa_67 = fadd ssa_61, ssa_64

	.reg .f32 %ssa_68;
	mul.f32 %ssa_68, %ssa_52_0, %ssa_35; // vec1 32 ssa_68 = fmul ssa_52.x, ssa_35

	.reg .f32 %ssa_69;
	mul.f32 %ssa_69, %ssa_52_1, %ssa_35; // vec1 32 ssa_69 = fmul ssa_52.y, ssa_35

	.reg .f32 %ssa_70;
	mul.f32 %ssa_70, %ssa_52_2, %ssa_35; // vec1 32 ssa_70 = fmul ssa_52.z, ssa_35

	.reg .f32 %ssa_71;
	add.f32 %ssa_71, %ssa_65, %ssa_68;	// vec1 32 ssa_71 = fadd ssa_65, ssa_68

	.reg .f32 %ssa_72;
	add.f32 %ssa_72, %ssa_66, %ssa_69;	// vec1 32 ssa_72 = fadd ssa_66, ssa_69

	.reg .f32 %ssa_73;
	add.f32 %ssa_73, %ssa_67, %ssa_70;	// vec1 32 ssa_73 = fadd ssa_67, ssa_70

	.reg .f32 %ssa_74;
	mul.f32 %ssa_74, %ssa_73, %ssa_73;	// vec1 32 ssa_74 = fmul ssa_73, ssa_73

	.reg .f32 %ssa_75;
	mul.f32 %ssa_75, %ssa_72, %ssa_72;	// vec1 32 ssa_75 = fmul ssa_72, ssa_72

	.reg .f32 %ssa_76;
	add.f32 %ssa_76, %ssa_74, %ssa_75;	// vec1 32 ssa_76 = fadd ssa_74, ssa_75

	.reg .f32 %ssa_77;
	mul.f32 %ssa_77, %ssa_71, %ssa_71;	// vec1 32 ssa_77 = fmul ssa_71, ssa_71

	.reg .f32 %ssa_78;
	add.f32 %ssa_78, %ssa_76, %ssa_77;	// vec1 32 ssa_78 = fadd ssa_76, ssa_77

	.reg .f32 %ssa_79;
	rsqrt.approx.f32 %ssa_79, %ssa_78;	// vec1 32 ssa_79 = frsq ssa_78

	.reg .f32 %ssa_80;
	mul.f32 %ssa_80, %ssa_71, %ssa_79;	// vec1 32 ssa_80 = fmul ssa_71, ssa_79

	.reg .f32 %ssa_81;
	mul.f32 %ssa_81, %ssa_72, %ssa_79;	// vec1 32 ssa_81 = fmul ssa_72, ssa_79

	.reg .f32 %ssa_82;
	mul.f32 %ssa_82, %ssa_73, %ssa_79;	// vec1 32 ssa_82 = fmul ssa_73, ssa_79

	.reg .f32 %ssa_83;
	mul.f32 %ssa_83, %ssa_46_0, %ssa_82; // vec1 32 ssa_83 = fmul ssa_46.x, ssa_82

	.reg .f32 %ssa_84;
	mul.f32 %ssa_84, %ssa_46_1, %ssa_82; // vec1 32 ssa_84 = fmul ssa_46.y, ssa_82

	.reg .f32 %ssa_85;
	mul.f32 %ssa_85, %ssa_46_2, %ssa_82; // vec1 32 ssa_85 = fmul ssa_46.z, ssa_82

	.reg .f32 %ssa_86;
	mul.f32 %ssa_86, %ssa_46_3, %ssa_82; // vec1 32 ssa_86 = fmul ssa_46.w, ssa_82

	.reg .f32 %ssa_87;
	mul.f32 %ssa_87, %ssa_44_0, %ssa_81; // vec1 32 ssa_87 = fmul ssa_44.x, ssa_81

	.reg .f32 %ssa_88;
	mul.f32 %ssa_88, %ssa_44_1, %ssa_81; // vec1 32 ssa_88 = fmul ssa_44.y, ssa_81

	.reg .f32 %ssa_89;
	mul.f32 %ssa_89, %ssa_44_2, %ssa_81; // vec1 32 ssa_89 = fmul ssa_44.z, ssa_81

	.reg .f32 %ssa_90;
	mul.f32 %ssa_90, %ssa_44_3, %ssa_81; // vec1 32 ssa_90 = fmul ssa_44.w, ssa_81

	.reg .f32 %ssa_91;
	add.f32 %ssa_91, %ssa_83, %ssa_87;	// vec1 32 ssa_91 = fadd ssa_83, ssa_87

	.reg .f32 %ssa_92;
	add.f32 %ssa_92, %ssa_84, %ssa_88;	// vec1 32 ssa_92 = fadd ssa_84, ssa_88

	.reg .f32 %ssa_93;
	add.f32 %ssa_93, %ssa_85, %ssa_89;	// vec1 32 ssa_93 = fadd ssa_85, ssa_89

	.reg .f32 %ssa_94;
	add.f32 %ssa_94, %ssa_86, %ssa_90;	// vec1 32 ssa_94 = fadd ssa_86, ssa_90

	.reg .f32 %ssa_95;
	mul.f32 %ssa_95, %ssa_42_0, %ssa_80; // vec1 32 ssa_95 = fmul ssa_42.x, ssa_80

	.reg .f32 %ssa_96;
	mul.f32 %ssa_96, %ssa_42_1, %ssa_80; // vec1 32 ssa_96 = fmul ssa_42.y, ssa_80

	.reg .f32 %ssa_97;
	mul.f32 %ssa_97, %ssa_42_2, %ssa_80; // vec1 32 ssa_97 = fmul ssa_42.z, ssa_80

	.reg .f32 %ssa_98;
	mul.f32 %ssa_98, %ssa_42_3, %ssa_80; // vec1 32 ssa_98 = fmul ssa_42.w, ssa_80

	.reg .f32 %ssa_99;
	add.f32 %ssa_99, %ssa_91, %ssa_95;	// vec1 32 ssa_99 = fadd ssa_91, ssa_95

	.reg .f32 %ssa_100;
	add.f32 %ssa_100, %ssa_92, %ssa_96;	// vec1 32 ssa_100 = fadd ssa_92, ssa_96

	.reg .f32 %ssa_101;
	add.f32 %ssa_101, %ssa_93, %ssa_97;	// vec1 32 ssa_101 = fadd ssa_93, ssa_97

	.reg .f32 %ssa_102;
	add.f32 %ssa_102, %ssa_94, %ssa_98;	// vec1 32 ssa_102 = fadd ssa_94, ssa_98

	.reg .f32 %ssa_103;
	mov.f32 %ssa_103, %ssa_49_0; // vec1 32 ssa_103 = mov ssa_49.x

	.reg .f32 %ssa_104;
	mov.f32 %ssa_104, %ssa_49_1; // vec1 32 ssa_104 = mov ssa_49.y

	.reg .f32 %ssa_105;
	mov.f32 %ssa_105, %ssa_49_2; // vec1 32 ssa_105 = mov ssa_49.z

	mov.b32 %ssa_106, %ssa_105; // vec1 32 ssa_106 = phi block_0: ssa_105, block_16: ssa_351
	mov.b32 %ssa_107, %ssa_104; // vec1 32 ssa_107 = phi block_0: ssa_104, block_16: ssa_350
	mov.b32 %ssa_108, %ssa_103; // vec1 32 ssa_108 = phi block_0: ssa_103, block_16: ssa_349
	mov.b32 %ssa_109, %ssa_102; // vec1 32 ssa_109 = phi block_0: ssa_102, block_16: ssa_348
	mov.b32 %ssa_110, %ssa_101; // vec1 32 ssa_110 = phi block_0: ssa_101, block_16: ssa_347
	mov.b32 %ssa_111, %ssa_100; // vec1 32 ssa_111 = phi block_0: ssa_100, block_16: ssa_346
	mov.b32 %ssa_112, %ssa_99; // vec1 32 ssa_112 = phi block_0: ssa_99, block_16: ssa_345
	mov.b32 %ssa_113, %ssa_5_bits; // vec1 32 ssa_113 = phi block_0: ssa_5, block_16: ssa_344
	mov.b32 %ssa_114, %ssa_5_bits; // vec1 32 ssa_114 = phi block_0: ssa_5, block_16: ssa_343
	mov.b32 %ssa_115, %ssa_5_bits; // vec1 32 ssa_115 = phi block_0: ssa_5, block_16: ssa_342
	mov.b32 %ssa_116, %ssa_5_bits; // vec1 32 ssa_116 = phi block_0: ssa_5, block_16: ssa_341
	mov.b32 %ssa_117, %ssa_5_bits; // vec1 32 ssa_117 = phi block_0: ssa_5, block_16: ssa_340
	mov.s32 %ssa_118, %ssa_5_bits; // vec1 32 ssa_118 = phi block_0: ssa_5, block_16: ssa_352
	mov.f32 %ssa_119, %ssa_5; // vec1 32 ssa_119 = phi block_0: ssa_5, block_16: ssa_354
	// succs: block_1 
	// end_block block_0:
	loop_0: 
		// start_block block_1:
		// preds: block_0 block_16 














		.reg .pred %ssa_120;
		setp.lt.u32 %ssa_120, %ssa_118, %ssa_15_bits; // vec1 1 ssa_120 = ult ssa_118, ssa_15

		.reg .pred %ssa_121;
		setp.lt.u32 %ssa_121, %ssa_117, %ssa_14_bits; // vec1 1 ssa_121 = ult ssa_117, ssa_14

		.reg .pred %ssa_122;
		and.pred %ssa_122, %ssa_120, %ssa_121;	// vec1 1 ssa_122 = iand ssa_120, ssa_121

		.reg .pred %ssa_123;
		setp.lt.f32 %ssa_123, %ssa_113, %ssa_13;	// vec1 1 ssa_123 = flt! ssa_113, ssa_13

		.reg .pred %ssa_124;
		and.pred %ssa_124, %ssa_122, %ssa_123;	// vec1 1 ssa_124 = iand ssa_122, ssa_123

		.reg .pred %ssa_125;
		setp.lt.f32 %ssa_125, %ssa_119, %ssa_12;	// vec1 1 ssa_125 = flt! ssa_119, ssa_12

		.reg .pred %ssa_126;
		and.pred %ssa_126, %ssa_124, %ssa_125;	// vec1 1 ssa_126 = iand ssa_124, ssa_125

		// succs: block_2 block_3 
		// end_block block_1:
		//if
		@!%ssa_126 bra else_0;
		
			// start_block block_2:
			// preds: block_1 
			// succs: block_4 
			// end_block block_2:
			bra end_if_0;
		
		else_0: 
			// start_block block_3:
			// preds: block_1 
			bra loop_0_exit;

			// succs: block_17 
			// end_block block_3:
		end_if_0:
		// start_block block_4:
		// preds: block_2 
		.reg .u32 %ssa_127;
		and.b32 %ssa_127, %ssa_20_0, %ssa_1; // vec1 32 ssa_127 = iand ssa_20.x, ssa_1

		.reg .pred %ssa_128;
		setp.eq.s32 %ssa_128, %ssa_127, %ssa_5_bits; // vec1 1 ssa_128 = ieq ssa_127, ssa_5

		// succs: block_5 block_6 
		// end_block block_4:
		//if
		@!%ssa_128 bra else_1;
		
			// start_block block_5:
			// preds: block_4 
			.reg .b64 %ssa_129;
			load_vulkan_descriptor %ssa_129, 0, 0, 1000150000; // vec1 64 ssa_129 = intrinsic vulkan_resource_index (%ssa_5) (0, 0, 1000150000) /* desc_set=0 */ /* binding=0 */ /* desc_type=accel-struct */

			.reg .b64 %ssa_130;
			mov.b64 %ssa_130, %ssa_129; // vec1 64 ssa_130 = intrinsic load_vulkan_descriptor (%ssa_129) (1000150000) /* desc_type=accel-struct */

			.reg .b32 %ssa_131_0;
			.reg .b32 %ssa_131_1;
			.reg .b32 %ssa_131_2;
			.reg .b32 %ssa_131_3;
			mov.b32 %ssa_131_0, %ssa_108;
			mov.b32 %ssa_131_1, %ssa_107;
			mov.b32 %ssa_131_2, %ssa_106; // vec3 32 ssa_131 = vec3 ssa_108, ssa_107, ssa_106

			.reg .b32 %ssa_132_0;
			.reg .b32 %ssa_132_1;
			.reg .b32 %ssa_132_2;
			.reg .b32 %ssa_132_3;
			mov.b32 %ssa_132_0, %ssa_112;
			mov.b32 %ssa_132_1, %ssa_111;
			mov.b32 %ssa_132_2, %ssa_110; // vec3 32 ssa_132 = vec3 ssa_112, ssa_111, ssa_110

			.reg .b64 %ssa_133;
	mov.b64 %ssa_133, %hitValue; // vec1 32 ssa_133 = deref_var &hitValue (function_temp Payload) 

			.reg .u32 %traversal_finished_0;
			trace_ray %ssa_130, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_131_0, %ssa_131_1, %ssa_131_2, %ssa_19, %ssa_132_0, %ssa_132_1, %ssa_132_2, %ssa_17, %traversal_finished_0; // intrinsic trace_ray (%ssa_130, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_131, %ssa_19, %ssa_132, %ssa_17, %ssa_133) ()

			.reg .u32 %intersection_counter_0;
			mov.u32 %intersection_counter_0, 0;
			intersection_loop_0:
			.reg .pred %intersections_exit_0;
			intersection_exit.pred %intersections_exit_0, %intersection_counter_0, %traversal_finished_0;
			@%intersections_exit_0 bra exit_intersection_label_0;
			.reg .pred %run_intersection_0;
			run_intersection.pred %run_intersection_0, %intersection_counter_0, %traversal_finished_0;
			@!%run_intersection_0 bra skip_intersection_label_0;
			call_intersection_shader %intersection_counter_0;
			skip_intersection_label_0:
			add.u32 %intersection_counter_0, %intersection_counter_0, 1;
			bra intersection_loop_0;
			exit_intersection_label_0:

			.reg .pred %hit_geometry_0;
			hit_geometry.pred %hit_geometry_0, %traversal_finished_0;

			@!%hit_geometry_0 bra exit_closest_hit_label_0;
			.reg .u32 %closest_hit_shaderID_0;
			get_closest_hit_shaderID %closest_hit_shaderID_0;
			.reg .pred %skip_closest_hit_2_0;
			setp.ne.u32 %skip_closest_hit_2_0, %closest_hit_shaderID_0, 2;
			@%skip_closest_hit_2_0 bra skip_closest_hit_label_2_0;
			call_closest_hit_shader 2;
			skip_closest_hit_label_2_0:
			exit_closest_hit_label_0:

			@%hit_geometry_0 bra skip_miss_label_0;
			call_miss_shader ;
			skip_miss_label_0:

			end_trace_ray ;

			// succs: block_7 
			// end_block block_5:
			bra end_if_1;
		
		else_1: 
			// start_block block_6:
			// preds: block_4 
			.reg .b64 %ssa_134;
			load_vulkan_descriptor %ssa_134, 0, 0, 1000150000; // vec1 64 ssa_134 = intrinsic vulkan_resource_index (%ssa_5) (0, 0, 1000150000) /* desc_set=0 */ /* binding=0 */ /* desc_type=accel-struct */

			.reg .b64 %ssa_135;
			mov.b64 %ssa_135, %ssa_134; // vec1 64 ssa_135 = intrinsic load_vulkan_descriptor (%ssa_134) (1000150000) /* desc_type=accel-struct */

			.reg .b32 %ssa_136_0;
			.reg .b32 %ssa_136_1;
			.reg .b32 %ssa_136_2;
			.reg .b32 %ssa_136_3;
			mov.b32 %ssa_136_0, %ssa_108;
			mov.b32 %ssa_136_1, %ssa_107;
			mov.b32 %ssa_136_2, %ssa_106; // vec3 32 ssa_136 = vec3 ssa_108, ssa_107, ssa_106

			.reg .b32 %ssa_137_0;
			.reg .b32 %ssa_137_1;
			.reg .b32 %ssa_137_2;
			.reg .b32 %ssa_137_3;
			mov.b32 %ssa_137_0, %ssa_112;
			mov.b32 %ssa_137_1, %ssa_111;
			mov.b32 %ssa_137_2, %ssa_110; // vec3 32 ssa_137 = vec3 ssa_112, ssa_111, ssa_110

			.reg .b64 %ssa_138;
	mov.b64 %ssa_138, %hitValue; // vec1 32 ssa_138 = deref_var &hitValue (function_temp Payload) 

			.reg .u32 %traversal_finished_1;
			trace_ray %ssa_135, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_136_0, %ssa_136_1, %ssa_136_2, %ssa_18, %ssa_137_0, %ssa_137_1, %ssa_137_2, %ssa_16, %traversal_finished_1; // intrinsic trace_ray (%ssa_135, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_136, %ssa_18, %ssa_137, %ssa_16, %ssa_138) ()

			.reg .u32 %intersection_counter_1;
			mov.u32 %intersection_counter_1, 0;
			intersection_loop_1:
			.reg .pred %intersections_exit_1;
			intersection_exit.pred %intersections_exit_1, %intersection_counter_1, %traversal_finished_1;
			@%intersections_exit_1 bra exit_intersection_label_1;
			.reg .pred %run_intersection_1;
			run_intersection.pred %run_intersection_1, %intersection_counter_1, %traversal_finished_1;
			@!%run_intersection_1 bra skip_intersection_label_1;
			call_intersection_shader %intersection_counter_1;
			skip_intersection_label_1:
			add.u32 %intersection_counter_1, %intersection_counter_1, 1;
			bra intersection_loop_1;
			exit_intersection_label_1:

			.reg .pred %hit_geometry_1;
			hit_geometry.pred %hit_geometry_1, %traversal_finished_1;

			@!%hit_geometry_1 bra exit_closest_hit_label_1;
			.reg .u32 %closest_hit_shaderID_1;
			get_closest_hit_shaderID %closest_hit_shaderID_1;
			.reg .pred %skip_closest_hit_2_1;
			setp.ne.u32 %skip_closest_hit_2_1, %closest_hit_shaderID_1, 2;
			@%skip_closest_hit_2_1 bra skip_closest_hit_label_2_1;
			call_closest_hit_shader 2;
			skip_closest_hit_label_2_1:
			exit_closest_hit_label_1:

			@%hit_geometry_1 bra skip_miss_label_1;
			call_miss_shader ;
			skip_miss_label_1:

			end_trace_ray ;

			// succs: block_7 
			// end_block block_6:
		end_if_1:
		// start_block block_7:
		// preds: block_5 block_6 
		.reg .b64 %ssa_139;
	mov.b64 %ssa_139, %hitValue; // vec1 32 ssa_139 = deref_var &hitValue (function_temp Payload) 

		.reg .b64 %ssa_140;
	add.u64 %ssa_140, %ssa_139, 16; // vec1 32 ssa_140 = deref_struct &ssa_139->field1 (function_temp vec4) /* &hitValue.field1 */

		.reg .f32 %ssa_141_0;
		.reg .f32 %ssa_141_1;
		.reg .f32 %ssa_141_2;
		.reg .f32 %ssa_141_3;
		ld.global.f32 %ssa_141_0, [%ssa_140 + 0];
		ld.global.f32 %ssa_141_1, [%ssa_140 + 4];
		ld.global.f32 %ssa_141_2, [%ssa_140 + 8];
		ld.global.f32 %ssa_141_3, [%ssa_140 + 12];
// vec4 32 ssa_141 = intrinsic load_deref (%ssa_140) (0) /* access=0 */


		.reg .u32 %ssa_142;
		cvt.rni.u32.f32 %ssa_142, %ssa_141_3; // vec1 32 ssa_142 = f2u32 ssa_141.w

		.reg .b64 %ssa_143;
	add.u64 %ssa_143, %ssa_139, 32; // vec1 32 ssa_143 = deref_struct &ssa_139->field2 (function_temp vec4) /* &hitValue.field2 */

		.reg .f32 %ssa_144_0;
		.reg .f32 %ssa_144_1;
		.reg .f32 %ssa_144_2;
		.reg .f32 %ssa_144_3;
		ld.global.f32 %ssa_144_0, [%ssa_143 + 0];
		ld.global.f32 %ssa_144_1, [%ssa_143 + 4];
		ld.global.f32 %ssa_144_2, [%ssa_143 + 8];
		ld.global.f32 %ssa_144_3, [%ssa_143 + 12];
// vec4 32 ssa_144 = intrinsic load_deref (%ssa_143) (0) /* access=0 */


		.reg .pred %ssa_145;
		setp.eq.s32 %ssa_145, %ssa_142, %ssa_5_bits; // vec1 1 ssa_145 = ieq ssa_142, ssa_5

		// succs: block_8 block_9 
		// end_block block_7:
		//if
		@!%ssa_145 bra else_2;
		
			// start_block block_8:
			// preds: block_7 
			.reg .b64 %ssa_146;
	add.u64 %ssa_146, %ssa_139, 0; // vec1 32 ssa_146 = deref_struct &ssa_139->field0 (function_temp vec4) /* &hitValue.field0 */

			.reg .f32 %ssa_147_0;
			.reg .f32 %ssa_147_1;
			.reg .f32 %ssa_147_2;
			.reg .f32 %ssa_147_3;
			ld.global.f32 %ssa_147_0, [%ssa_146 + 0];
			ld.global.f32 %ssa_147_1, [%ssa_146 + 4];
			ld.global.f32 %ssa_147_2, [%ssa_146 + 8];
			ld.global.f32 %ssa_147_3, [%ssa_146 + 12];
// vec4 32 ssa_147 = intrinsic load_deref (%ssa_146) (0) /* access=0 */


			.reg .f32 %ssa_148;
			neg.f32 %ssa_148, %ssa_141_1; // vec1 32 ssa_148 = fneg ssa_141.y

			.reg .f32 %ssa_149;
			add.f32 %ssa_149, %ssa_11, %ssa_148;	// vec1 32 ssa_149 = fadd ssa_11, ssa_148

			.reg .f32 %ssa_150;
			mul.f32 %ssa_150, %ssa_141_2, %ssa_141_2; // vec1 32 ssa_150 = fmul ssa_141.z, ssa_141.z

			.reg .f32 %ssa_151;
			mul.f32 %ssa_151, %ssa_149, %ssa_149;	// vec1 32 ssa_151 = fmul ssa_149, ssa_149

			.reg .f32 %ssa_152;
			add.f32 %ssa_152, %ssa_150, %ssa_151;	// vec1 32 ssa_152 = fadd ssa_150, ssa_151

			.reg .f32 %ssa_153;
			mul.f32 %ssa_153, %ssa_141_0, %ssa_141_0; // vec1 32 ssa_153 = fmul ssa_141.x, ssa_141.x

			.reg .f32 %ssa_154;
			add.f32 %ssa_154, %ssa_152, %ssa_153;	// vec1 32 ssa_154 = fadd ssa_152, ssa_153

			.reg .f32 %ssa_155;
			sqrt.approx.f32 %ssa_155, %ssa_154;	// vec1 32 ssa_155 = fsqrt ssa_154

			.reg .f32 %ssa_156;
			rsqrt.approx.f32 %ssa_156, %ssa_154;	// vec1 32 ssa_156 = frsq ssa_154

			.reg .f32 %ssa_157;
			mul.f32 %ssa_157, %ssa_141_0, %ssa_156; // vec1 32 ssa_157 = fmul ssa_141.x, ssa_156

			.reg .f32 %ssa_158;
			neg.f32 %ssa_158, %ssa_157;	// vec1 32 ssa_158 = fneg ssa_157

			.reg .f32 %ssa_159;
			mul.f32 %ssa_159, %ssa_149, %ssa_156;	// vec1 32 ssa_159 = fmul ssa_149, ssa_156

			.reg .f32 %ssa_160;
			mul.f32 %ssa_160, %ssa_141_2, %ssa_156; // vec1 32 ssa_160 = fmul ssa_141.z, ssa_156

			.reg .f32 %ssa_161;
			neg.f32 %ssa_161, %ssa_160;	// vec1 32 ssa_161 = fneg ssa_160

			.reg .b64 %ssa_162;
			load_vulkan_descriptor %ssa_162, 0, 0, 1000150000; // vec1 64 ssa_162 = intrinsic vulkan_resource_index (%ssa_5) (0, 0, 1000150000) /* desc_set=0 */ /* binding=0 */ /* desc_type=accel-struct */

			.reg .b64 %ssa_163;
			mov.b64 %ssa_163, %ssa_162; // vec1 64 ssa_163 = intrinsic load_vulkan_descriptor (%ssa_162) (1000150000) /* desc_type=accel-struct */

			.reg .f32 %ssa_164_0;
			.reg .f32 %ssa_164_1;
			.reg .f32 %ssa_164_2;
			.reg .f32 %ssa_164_3;
			mov.f32 %ssa_164_0, %ssa_141_0;
			mov.f32 %ssa_164_1, %ssa_141_1;
			mov.f32 %ssa_164_2, %ssa_141_2; // vec3 32 ssa_164 = vec3 ssa_141.x, ssa_141.y, ssa_141.z

			.reg .f32 %ssa_165_0;
			.reg .f32 %ssa_165_1;
			.reg .f32 %ssa_165_2;
			.reg .f32 %ssa_165_3;
			mov.f32 %ssa_165_0, %ssa_158;
			mov.f32 %ssa_165_1, %ssa_159;
			mov.f32 %ssa_165_2, %ssa_161; // vec3 32 ssa_165 = vec3 ssa_158, ssa_159, ssa_161

			.reg .u32 %traversal_finished_2;
			trace_ray %ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164_0, %ssa_164_1, %ssa_164_2, %ssa_19, %ssa_165_0, %ssa_165_1, %ssa_165_2, %ssa_17, %traversal_finished_2; // intrinsic trace_ray (%ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164, %ssa_19, %ssa_165, %ssa_17, %ssa_139) ()

			.reg .u32 %intersection_counter_2;
			mov.u32 %intersection_counter_2, 0;
			intersection_loop_2:
			.reg .pred %intersections_exit_2;
			intersection_exit.pred %intersections_exit_2, %intersection_counter_2, %traversal_finished_2;
			@%intersections_exit_2 bra exit_intersection_label_2;
			.reg .pred %run_intersection_2;
			run_intersection.pred %run_intersection_2, %intersection_counter_2, %traversal_finished_2;
			@!%run_intersection_2 bra skip_intersection_label_2;
			call_intersection_shader %intersection_counter_2;
			skip_intersection_label_2:
			add.u32 %intersection_counter_2, %intersection_counter_2, 1;
			bra intersection_loop_2;
			exit_intersection_label_2:

			.reg .pred %hit_geometry_2;
			hit_geometry.pred %hit_geometry_2, %traversal_finished_2;

			@!%hit_geometry_2 bra exit_closest_hit_label_2;
			.reg .u32 %closest_hit_shaderID_2;
			get_closest_hit_shaderID %closest_hit_shaderID_2;
			.reg .pred %skip_closest_hit_2_2;
			setp.ne.u32 %skip_closest_hit_2_2, %closest_hit_shaderID_2, 2;
			@%skip_closest_hit_2_2 bra skip_closest_hit_label_2_2;
			call_closest_hit_shader 2;
			skip_closest_hit_label_2_2:
			exit_closest_hit_label_2:

			@%hit_geometry_2 bra skip_miss_label_2;
			call_miss_shader ;
			skip_miss_label_2:

			end_trace_ray ;

			.reg .f32 %ssa_166_0;
			.reg .f32 %ssa_166_1;
			.reg .f32 %ssa_166_2;
			.reg .f32 %ssa_166_3;
			ld.global.f32 %ssa_166_0, [%ssa_143 + 0];
			ld.global.f32 %ssa_166_1, [%ssa_143 + 4];
			ld.global.f32 %ssa_166_2, [%ssa_143 + 8];
			ld.global.f32 %ssa_166_3, [%ssa_143 + 12];
// vec4 32 ssa_166 = intrinsic load_deref (%ssa_143) (0) /* access=0 */


			.reg .pred %ssa_167;
			setp.lt.f32 %ssa_167, %ssa_166_3, %ssa_155; // vec1 1 ssa_167 = flt! ssa_166.w, ssa_155

			.reg  .f32 %ssa_168;
			selp.f32 %ssa_168, %ssa_8_bits, %ssa_6_bits, %ssa_167; // vec1 32 ssa_168 = bcsel ssa_167, ssa_8, ssa_6

			.reg .f32 %ssa_169;
			max.f32 %ssa_169, %ssa_168, %const0_f32;
			min.f32 %ssa_169, %ssa_169, %const1_f32;

			.reg .f32 %ssa_170;
			mul.f32 %ssa_170, %ssa_147_0, %ssa_169; // vec1 32 ssa_170 = fmul ssa_147.x, ssa_169

			.reg .f32 %ssa_171;
			mul.f32 %ssa_171, %ssa_147_1, %ssa_169; // vec1 32 ssa_171 = fmul ssa_147.y, ssa_169

			.reg .f32 %ssa_172;
			mul.f32 %ssa_172, %ssa_147_2, %ssa_169; // vec1 32 ssa_172 = fmul ssa_147.z, ssa_169

			.reg .f32 %ssa_173;
			abs.f32 %ssa_173, %ssa_144_2; // vec1 32 ssa_173 = fabs ssa_144.z

			.reg .pred %ssa_174;
			setp.lt.f32 %ssa_174, %ssa_9, %ssa_173;	// vec1 1 ssa_174 = flt! ssa_9, ssa_173

			.reg .f32 %ssa_175;
	mov.f32 %ssa_175, 0F80000000; // vec1 32 ssa_175 = load_const (0x80000000 /* -0.000000 */)
			.reg .b32 %ssa_175_bits;
	mov.f32 %ssa_175_bits, 0F80000000;

			.reg .f32 %ssa_176;
			neg.f32 %ssa_176, %ssa_144_1; // vec1 32 ssa_176 = fneg ssa_144.y

			.reg .f32 %ssa_177;
			neg.f32 %ssa_177, %ssa_144_0; // vec1 32 ssa_177 = fneg ssa_144.x

			.reg  .f32 %ssa_178;
			selp.f32 %ssa_178, %ssa_175_bits, %ssa_144_1, %ssa_174; // vec1 32 ssa_178 = bcsel ssa_174, ssa_175, ssa_144.y

			.reg  .f32 %ssa_179;
			selp.f32 %ssa_179, %ssa_144_2, %ssa_177, %ssa_174; // vec1 32 ssa_179 = bcsel ssa_174, ssa_144.z, ssa_177

			.reg  .f32 %ssa_180;
			selp.f32 %ssa_180, %ssa_176, %ssa_175_bits, %ssa_174; // vec1 32 ssa_180 = bcsel ssa_174, ssa_176, ssa_175

			.reg .f32 %ssa_181;
			mul.f32 %ssa_181, %ssa_144_2, %ssa_179; // vec1 32 ssa_181 = fmul ssa_144.z, ssa_179

			.reg .f32 %ssa_182;
			mul.f32 %ssa_182, %ssa_144_0, %ssa_180; // vec1 32 ssa_182 = fmul ssa_144.x, ssa_180

			.reg .f32 %ssa_183;
			mul.f32 %ssa_183, %ssa_144_1, %ssa_178; // vec1 32 ssa_183 = fmul ssa_144.y, ssa_178

			.reg .f32 %ssa_184;
			mul.f32 %ssa_184, %ssa_144_1, %ssa_180; // vec1 32 ssa_184 = fmul ssa_144.y, ssa_180

			.reg .f32 %ssa_185;
			mul.f32 %ssa_185, %ssa_144_2, %ssa_178; // vec1 32 ssa_185 = fmul ssa_144.z, ssa_178

			.reg .f32 %ssa_186;
			mul.f32 %ssa_186, %ssa_144_0, %ssa_179; // vec1 32 ssa_186 = fmul ssa_144.x, ssa_179

			.reg .f32 %ssa_187;
			neg.f32 %ssa_187, %ssa_181;	// vec1 32 ssa_187 = fneg ssa_181

			.reg .f32 %ssa_188;
			add.f32 %ssa_188, %ssa_184, %ssa_187;	// vec1 32 ssa_188 = fadd ssa_184, ssa_187

			.reg .f32 %ssa_189;
			neg.f32 %ssa_189, %ssa_182;	// vec1 32 ssa_189 = fneg ssa_182

			.reg .f32 %ssa_190;
			add.f32 %ssa_190, %ssa_185, %ssa_189;	// vec1 32 ssa_190 = fadd ssa_185, ssa_189

			.reg .f32 %ssa_191;
			neg.f32 %ssa_191, %ssa_183;	// vec1 32 ssa_191 = fneg ssa_183

			.reg .f32 %ssa_192;
			add.f32 %ssa_192, %ssa_186, %ssa_191;	// vec1 32 ssa_192 = fadd ssa_186, ssa_191

			.reg .f32 %ssa_193;
	mov.f32 %ssa_193, 0Fbf000000; // vec1 32 ssa_193 = load_const (0xbf000000 /* -0.500000 */)
			.reg .b32 %ssa_193_bits;
	mov.f32 %ssa_193_bits, 0Fbf000000;

			.reg .f32 %ssa_194;
	mov.f32 %ssa_194, 0F3effffec; // vec1 32 ssa_194 = load_const (0x3effffec /* 0.499999 */)
			.reg .b32 %ssa_194_bits;
	mov.f32 %ssa_194_bits, 0F3effffec;

			.reg .f32 %ssa_195;
	mov.f32 %ssa_195, 0F3f3504fb; // vec1 32 ssa_195 = load_const (0x3f3504fb /* 0.707107 */)
			.reg .b32 %ssa_195_bits;
	mov.f32 %ssa_195_bits, 0F3f3504fb;

			.reg .f32 %ssa_196;
			mul.f32 %ssa_196, %ssa_178, %ssa_193;	// vec1 32 ssa_196 = fmul ssa_178, ssa_193

			.reg .f32 %ssa_197;
			mul.f32 %ssa_197, %ssa_179, %ssa_193;	// vec1 32 ssa_197 = fmul ssa_179, ssa_193

			.reg .f32 %ssa_198;
			mul.f32 %ssa_198, %ssa_180, %ssa_193;	// vec1 32 ssa_198 = fmul ssa_180, ssa_193

			.reg .f32 %ssa_199;
			mul.f32 %ssa_199, %ssa_188, %ssa_194;	// vec1 32 ssa_199 = fmul ssa_188, ssa_194

			.reg .f32 %ssa_200;
			mul.f32 %ssa_200, %ssa_190, %ssa_194;	// vec1 32 ssa_200 = fmul ssa_190, ssa_194

			.reg .f32 %ssa_201;
			mul.f32 %ssa_201, %ssa_192, %ssa_194;	// vec1 32 ssa_201 = fmul ssa_192, ssa_194

			.reg .f32 %ssa_202;
			add.f32 %ssa_202, %ssa_196, %ssa_199;	// vec1 32 ssa_202 = fadd ssa_196, ssa_199

			.reg .f32 %ssa_203;
			add.f32 %ssa_203, %ssa_197, %ssa_200;	// vec1 32 ssa_203 = fadd ssa_197, ssa_200

			.reg .f32 %ssa_204;
			add.f32 %ssa_204, %ssa_198, %ssa_201;	// vec1 32 ssa_204 = fadd ssa_198, ssa_201

			.reg .f32 %ssa_205;
			mul.f32 %ssa_205, %ssa_144_0, %ssa_195; // vec1 32 ssa_205 = fmul ssa_144.x, ssa_195

			.reg .f32 %ssa_206;
			mul.f32 %ssa_206, %ssa_144_1, %ssa_195; // vec1 32 ssa_206 = fmul ssa_144.y, ssa_195

			.reg .f32 %ssa_207;
			mul.f32 %ssa_207, %ssa_144_2, %ssa_195; // vec1 32 ssa_207 = fmul ssa_144.z, ssa_195

			.reg .f32 %ssa_208;
			add.f32 %ssa_208, %ssa_202, %ssa_205;	// vec1 32 ssa_208 = fadd ssa_202, ssa_205

			.reg .f32 %ssa_209;
			add.f32 %ssa_209, %ssa_203, %ssa_206;	// vec1 32 ssa_209 = fadd ssa_203, ssa_206

			.reg .f32 %ssa_210;
			add.f32 %ssa_210, %ssa_204, %ssa_207;	// vec1 32 ssa_210 = fadd ssa_204, ssa_207

			.reg .f32 %ssa_211_0;
			.reg .f32 %ssa_211_1;
			.reg .f32 %ssa_211_2;
			.reg .f32 %ssa_211_3;
			mov.f32 %ssa_211_0, %ssa_208;
			mov.f32 %ssa_211_1, %ssa_209;
			mov.f32 %ssa_211_2, %ssa_210; // vec3 32 ssa_211 = vec3 ssa_208, ssa_209, ssa_210

			.reg .u32 %traversal_finished_3;
			trace_ray %ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164_0, %ssa_164_1, %ssa_164_2, %ssa_19, %ssa_211_0, %ssa_211_1, %ssa_211_2, %ssa_17, %traversal_finished_3; // intrinsic trace_ray (%ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164, %ssa_19, %ssa_211, %ssa_17, %ssa_139) ()

			.reg .u32 %intersection_counter_3;
			mov.u32 %intersection_counter_3, 0;
			intersection_loop_3:
			.reg .pred %intersections_exit_3;
			intersection_exit.pred %intersections_exit_3, %intersection_counter_3, %traversal_finished_3;
			@%intersections_exit_3 bra exit_intersection_label_3;
			.reg .pred %run_intersection_3;
			run_intersection.pred %run_intersection_3, %intersection_counter_3, %traversal_finished_3;
			@!%run_intersection_3 bra skip_intersection_label_3;
			call_intersection_shader %intersection_counter_3;
			skip_intersection_label_3:
			add.u32 %intersection_counter_3, %intersection_counter_3, 1;
			bra intersection_loop_3;
			exit_intersection_label_3:

			.reg .pred %hit_geometry_3;
			hit_geometry.pred %hit_geometry_3, %traversal_finished_3;

			@!%hit_geometry_3 bra exit_closest_hit_label_3;
			.reg .u32 %closest_hit_shaderID_3;
			get_closest_hit_shaderID %closest_hit_shaderID_3;
			.reg .pred %skip_closest_hit_2_3;
			setp.ne.u32 %skip_closest_hit_2_3, %closest_hit_shaderID_3, 2;
			@%skip_closest_hit_2_3 bra skip_closest_hit_label_2_3;
			call_closest_hit_shader 2;
			skip_closest_hit_label_2_3:
			exit_closest_hit_label_3:

			@%hit_geometry_3 bra skip_miss_label_3;
			call_miss_shader ;
			skip_miss_label_3:

			end_trace_ray ;

			.reg .f32 %ssa_212_0;
			.reg .f32 %ssa_212_1;
			.reg .f32 %ssa_212_2;
			.reg .f32 %ssa_212_3;
			ld.global.f32 %ssa_212_0, [%ssa_143 + 0];
			ld.global.f32 %ssa_212_1, [%ssa_143 + 4];
			ld.global.f32 %ssa_212_2, [%ssa_143 + 8];
			ld.global.f32 %ssa_212_3, [%ssa_143 + 12];
// vec4 32 ssa_212 = intrinsic load_deref (%ssa_143) (0) /* access=0 */


			.reg .f32 %ssa_213;
			min.f32 %ssa_213, %ssa_212_3, %ssa_6; // vec1 32 ssa_213 = fmin ssa_212.w, ssa_6

			.reg .f32 %ssa_214;
	mov.f32 %ssa_214, 0F3f1999a3; // vec1 32 ssa_214 = load_const (0x3f1999a3 /* 0.600001 */)
			.reg .b32 %ssa_214_bits;
	mov.f32 %ssa_214_bits, 0F3f1999a3;

			.reg .f32 %ssa_215;
			mul.f32 %ssa_215, %ssa_213, %ssa_214;	// vec1 32 ssa_215 = fmul ssa_213, ssa_214

			.reg .f32 %ssa_216_0;
			.reg .f32 %ssa_216_1;
			.reg .f32 %ssa_216_2;
			.reg .f32 %ssa_216_3;
			mov.f32 %ssa_216_0, %ssa_144_0;
			mov.f32 %ssa_216_1, %ssa_144_1;
			mov.f32 %ssa_216_2, %ssa_144_2; // vec3 32 ssa_216 = vec3 ssa_144.x, ssa_144.y, ssa_144.z

			.reg .u32 %traversal_finished_4;
			trace_ray %ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164_0, %ssa_164_1, %ssa_164_2, %ssa_19, %ssa_216_0, %ssa_216_1, %ssa_216_2, %ssa_17, %traversal_finished_4; // intrinsic trace_ray (%ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164, %ssa_19, %ssa_216, %ssa_17, %ssa_139) ()

			.reg .u32 %intersection_counter_4;
			mov.u32 %intersection_counter_4, 0;
			intersection_loop_4:
			.reg .pred %intersections_exit_4;
			intersection_exit.pred %intersections_exit_4, %intersection_counter_4, %traversal_finished_4;
			@%intersections_exit_4 bra exit_intersection_label_4;
			.reg .pred %run_intersection_4;
			run_intersection.pred %run_intersection_4, %intersection_counter_4, %traversal_finished_4;
			@!%run_intersection_4 bra skip_intersection_label_4;
			call_intersection_shader %intersection_counter_4;
			skip_intersection_label_4:
			add.u32 %intersection_counter_4, %intersection_counter_4, 1;
			bra intersection_loop_4;
			exit_intersection_label_4:

			.reg .pred %hit_geometry_4;
			hit_geometry.pred %hit_geometry_4, %traversal_finished_4;

			@!%hit_geometry_4 bra exit_closest_hit_label_4;
			.reg .u32 %closest_hit_shaderID_4;
			get_closest_hit_shaderID %closest_hit_shaderID_4;
			.reg .pred %skip_closest_hit_2_4;
			setp.ne.u32 %skip_closest_hit_2_4, %closest_hit_shaderID_4, 2;
			@%skip_closest_hit_2_4 bra skip_closest_hit_label_2_4;
			call_closest_hit_shader 2;
			skip_closest_hit_label_2_4:
			exit_closest_hit_label_4:

			@%hit_geometry_4 bra skip_miss_label_4;
			call_miss_shader ;
			skip_miss_label_4:

			end_trace_ray ;

			.reg .f32 %ssa_217_0;
			.reg .f32 %ssa_217_1;
			.reg .f32 %ssa_217_2;
			.reg .f32 %ssa_217_3;
			ld.global.f32 %ssa_217_0, [%ssa_143 + 0];
			ld.global.f32 %ssa_217_1, [%ssa_143 + 4];
			ld.global.f32 %ssa_217_2, [%ssa_143 + 8];
			ld.global.f32 %ssa_217_3, [%ssa_143 + 12];
// vec4 32 ssa_217 = intrinsic load_deref (%ssa_143) (0) /* access=0 */


			.reg .f32 %ssa_218;
			min.f32 %ssa_218, %ssa_217_3, %ssa_6; // vec1 32 ssa_218 = fmin ssa_217.w, ssa_6

			.reg .f32 %ssa_219;
			add.f32 %ssa_219, %ssa_215, %ssa_218;	// vec1 32 ssa_219 = fadd ssa_215, ssa_218

			.reg .f32 %ssa_220;
	mov.f32 %ssa_220, 0Fbf3504ec; // vec1 32 ssa_220 = load_const (0xbf3504ec /* -0.707106 */)
			.reg .b32 %ssa_220_bits;
	mov.f32 %ssa_220_bits, 0Fbf3504ec;

			.reg .f32 %ssa_221;
			mul.f32 %ssa_221, %ssa_178, %ssa_220;	// vec1 32 ssa_221 = fmul ssa_178, ssa_220

			.reg .f32 %ssa_222;
			mul.f32 %ssa_222, %ssa_179, %ssa_220;	// vec1 32 ssa_222 = fmul ssa_179, ssa_220

			.reg .f32 %ssa_223;
			mul.f32 %ssa_223, %ssa_180, %ssa_220;	// vec1 32 ssa_223 = fmul ssa_180, ssa_220

			.reg .f32 %ssa_224;
			add.f32 %ssa_224, %ssa_221, %ssa_205;	// vec1 32 ssa_224 = fadd ssa_221, ssa_205

			.reg .f32 %ssa_225;
			add.f32 %ssa_225, %ssa_222, %ssa_206;	// vec1 32 ssa_225 = fadd ssa_222, ssa_206

			.reg .f32 %ssa_226;
			add.f32 %ssa_226, %ssa_223, %ssa_207;	// vec1 32 ssa_226 = fadd ssa_223, ssa_207

			.reg .f32 %ssa_227_0;
			.reg .f32 %ssa_227_1;
			.reg .f32 %ssa_227_2;
			.reg .f32 %ssa_227_3;
			mov.f32 %ssa_227_0, %ssa_224;
			mov.f32 %ssa_227_1, %ssa_225;
			mov.f32 %ssa_227_2, %ssa_226; // vec3 32 ssa_227 = vec3 ssa_224, ssa_225, ssa_226

			.reg .u32 %traversal_finished_5;
			trace_ray %ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164_0, %ssa_164_1, %ssa_164_2, %ssa_19, %ssa_227_0, %ssa_227_1, %ssa_227_2, %ssa_17, %traversal_finished_5; // intrinsic trace_ray (%ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164, %ssa_19, %ssa_227, %ssa_17, %ssa_139) ()

			.reg .u32 %intersection_counter_5;
			mov.u32 %intersection_counter_5, 0;
			intersection_loop_5:
			.reg .pred %intersections_exit_5;
			intersection_exit.pred %intersections_exit_5, %intersection_counter_5, %traversal_finished_5;
			@%intersections_exit_5 bra exit_intersection_label_5;
			.reg .pred %run_intersection_5;
			run_intersection.pred %run_intersection_5, %intersection_counter_5, %traversal_finished_5;
			@!%run_intersection_5 bra skip_intersection_label_5;
			call_intersection_shader %intersection_counter_5;
			skip_intersection_label_5:
			add.u32 %intersection_counter_5, %intersection_counter_5, 1;
			bra intersection_loop_5;
			exit_intersection_label_5:

			.reg .pred %hit_geometry_5;
			hit_geometry.pred %hit_geometry_5, %traversal_finished_5;

			@!%hit_geometry_5 bra exit_closest_hit_label_5;
			.reg .u32 %closest_hit_shaderID_5;
			get_closest_hit_shaderID %closest_hit_shaderID_5;
			.reg .pred %skip_closest_hit_2_5;
			setp.ne.u32 %skip_closest_hit_2_5, %closest_hit_shaderID_5, 2;
			@%skip_closest_hit_2_5 bra skip_closest_hit_label_2_5;
			call_closest_hit_shader 2;
			skip_closest_hit_label_2_5:
			exit_closest_hit_label_5:

			@%hit_geometry_5 bra skip_miss_label_5;
			call_miss_shader ;
			skip_miss_label_5:

			end_trace_ray ;

			.reg .f32 %ssa_228_0;
			.reg .f32 %ssa_228_1;
			.reg .f32 %ssa_228_2;
			.reg .f32 %ssa_228_3;
			ld.global.f32 %ssa_228_0, [%ssa_143 + 0];
			ld.global.f32 %ssa_228_1, [%ssa_143 + 4];
			ld.global.f32 %ssa_228_2, [%ssa_143 + 8];
			ld.global.f32 %ssa_228_3, [%ssa_143 + 12];
// vec4 32 ssa_228 = intrinsic load_deref (%ssa_143) (0) /* access=0 */


			.reg .f32 %ssa_229;
			min.f32 %ssa_229, %ssa_228_3, %ssa_6; // vec1 32 ssa_229 = fmin ssa_228.w, ssa_6

			.reg .f32 %ssa_230;
			mul.f32 %ssa_230, %ssa_229, %ssa_214;	// vec1 32 ssa_230 = fmul ssa_229, ssa_214

			.reg .f32 %ssa_231;
			add.f32 %ssa_231, %ssa_219, %ssa_230;	// vec1 32 ssa_231 = fadd ssa_219, ssa_230

			.reg .u32 %traversal_finished_6;
			trace_ray %ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164_0, %ssa_164_1, %ssa_164_2, %ssa_19, %ssa_216_0, %ssa_216_1, %ssa_216_2, %ssa_17, %traversal_finished_6; // intrinsic trace_ray (%ssa_163, %ssa_1, %ssa_7, %ssa_5, %ssa_5, %ssa_5, %ssa_164, %ssa_19, %ssa_216, %ssa_17, %ssa_139) ()

			.reg .u32 %intersection_counter_6;
			mov.u32 %intersection_counter_6, 0;
			intersection_loop_6:
			.reg .pred %intersections_exit_6;
			intersection_exit.pred %intersections_exit_6, %intersection_counter_6, %traversal_finished_6;
			@%intersections_exit_6 bra exit_intersection_label_6;
			.reg .pred %run_intersection_6;
			run_intersection.pred %run_intersection_6, %intersection_counter_6, %traversal_finished_6;
			@!%run_intersection_6 bra skip_intersection_label_6;
			call_intersection_shader %intersection_counter_6;
			skip_intersection_label_6:
			add.u32 %intersection_counter_6, %intersection_counter_6, 1;
			bra intersection_loop_6;
			exit_intersection_label_6:

			.reg .pred %hit_geometry_6;
			hit_geometry.pred %hit_geometry_6, %traversal_finished_6;

			@!%hit_geometry_6 bra exit_closest_hit_label_6;
			.reg .u32 %closest_hit_shaderID_6;
			get_closest_hit_shaderID %closest_hit_shaderID_6;
			.reg .pred %skip_closest_hit_2_6;
			setp.ne.u32 %skip_closest_hit_2_6, %closest_hit_shaderID_6, 2;
			@%skip_closest_hit_2_6 bra skip_closest_hit_label_2_6;
			call_closest_hit_shader 2;
			skip_closest_hit_label_2_6:
			exit_closest_hit_label_6:

			@%hit_geometry_6 bra skip_miss_label_6;
			call_miss_shader ;
			skip_miss_label_6:

			end_trace_ray ;

			.reg .f32 %ssa_232_0;
			.reg .f32 %ssa_232_1;
			.reg .f32 %ssa_232_2;
			.reg .f32 %ssa_232_3;
			ld.global.f32 %ssa_232_0, [%ssa_143 + 0];
			ld.global.f32 %ssa_232_1, [%ssa_143 + 4];
			ld.global.f32 %ssa_232_2, [%ssa_143 + 8];
			ld.global.f32 %ssa_232_3, [%ssa_143 + 12];
// vec4 32 ssa_232 = intrinsic load_deref (%ssa_143) (0) /* access=0 */


			.reg .f32 %ssa_233;
			min.f32 %ssa_233, %ssa_232_3, %ssa_6; // vec1 32 ssa_233 = fmin ssa_232.w, ssa_6

			.reg .f32 %ssa_234;
			add.f32 %ssa_234, %ssa_231, %ssa_233;	// vec1 32 ssa_234 = fadd ssa_231, ssa_233

			.reg .f32 %ssa_235;
	mov.f32 %ssa_235, 0F3e1ffffc; // vec1 32 ssa_235 = load_const (0x3e1ffffc /* 0.156250 */)
			.reg .b32 %ssa_235_bits;
	mov.f32 %ssa_235_bits, 0F3e1ffffc;

			.reg .f32 %ssa_236;
			mul.f32 %ssa_236, %ssa_234, %ssa_235;	// vec1 32 ssa_236 = fmul ssa_234, ssa_235

			.reg .f32 %ssa_237;
			mul.f32 %ssa_237, %ssa_236, %ssa_236;	// vec1 32 ssa_237 = fmul ssa_236, ssa_236

			.reg .f32 %ssa_238;
			max.f32 %ssa_238, %ssa_237, %const0_f32;
			min.f32 %ssa_238, %ssa_238, %const1_f32;

			.reg .f32 %ssa_239;
			mul.f32 %ssa_239, %ssa_170, %ssa_238;	// vec1 32 ssa_239 = fmul ssa_170, ssa_238

			.reg .f32 %ssa_240;
			mul.f32 %ssa_240, %ssa_171, %ssa_238;	// vec1 32 ssa_240 = fmul ssa_171, ssa_238

			.reg .f32 %ssa_241;
			mul.f32 %ssa_241, %ssa_172, %ssa_238;	// vec1 32 ssa_241 = fmul ssa_172, ssa_238

			.reg .f32 %ssa_242;
			neg.f32 %ssa_242, %ssa_113;	// vec1 32 ssa_242 = fneg ssa_113

			.reg .f32 %ssa_243;
			add.f32 %ssa_243, %ssa_3, %ssa_242;	// vec1 32 ssa_243 = fadd ssa_3, ssa_242

			.reg .f32 %ssa_244;
			max.f32 %ssa_244, %ssa_5, %ssa_243;	// vec1 32 ssa_244 = fmax ssa_5, ssa_243

			.reg .f32 %ssa_245;
			mul.f32 %ssa_245, %ssa_239, %ssa_244;	// vec1 32 ssa_245 = fmul ssa_239, ssa_244

			.reg .f32 %ssa_246;
			mul.f32 %ssa_246, %ssa_240, %ssa_244;	// vec1 32 ssa_246 = fmul ssa_240, ssa_244

			.reg .f32 %ssa_247;
			mul.f32 %ssa_247, %ssa_241, %ssa_244;	// vec1 32 ssa_247 = fmul ssa_241, ssa_244

			.reg .f32 %ssa_248;
			add.f32 %ssa_248, %ssa_116, %ssa_245;	// vec1 32 ssa_248 = fadd ssa_116, ssa_245

			.reg .f32 %ssa_249;
			add.f32 %ssa_249, %ssa_115, %ssa_246;	// vec1 32 ssa_249 = fadd ssa_115, ssa_246

			.reg .f32 %ssa_250;
			add.f32 %ssa_250, %ssa_114, %ssa_247;	// vec1 32 ssa_250 = fadd ssa_114, ssa_247

			.reg .f32 %ssa_251;
			add.f32 %ssa_251, %ssa_113, %ssa_244;	// vec1 32 ssa_251 = fadd ssa_113, ssa_244

	mov.b32 %ssa_340, %ssa_10_bits; // vec1 32 ssa_340 = phi block_8: ssa_10, block_15: ssa_117
			mov.b32 %ssa_341, %ssa_248; // vec1 32 ssa_341 = phi block_8: ssa_248, block_15: ssa_329
			mov.b32 %ssa_342, %ssa_249; // vec1 32 ssa_342 = phi block_8: ssa_249, block_15: ssa_330
			mov.b32 %ssa_343, %ssa_250; // vec1 32 ssa_343 = phi block_8: ssa_250, block_15: ssa_331
			mov.b32 %ssa_344, %ssa_251; // vec1 32 ssa_344 = phi block_8: ssa_251, block_15: ssa_332
		mov.b32 %ssa_345, %ssa_112; // vec1 32 ssa_345 = phi block_8: ssa_112, block_15: ssa_333
		mov.b32 %ssa_346, %ssa_111; // vec1 32 ssa_346 = phi block_8: ssa_111, block_15: ssa_334
		mov.b32 %ssa_347, %ssa_110; // vec1 32 ssa_347 = phi block_8: ssa_110, block_15: ssa_335
		mov.b32 %ssa_348, %ssa_109; // vec1 32 ssa_348 = phi block_8: ssa_109, block_15: ssa_336
		mov.b32 %ssa_349, %ssa_108; // vec1 32 ssa_349 = phi block_8: ssa_108, block_15: ssa_337
		mov.b32 %ssa_350, %ssa_107; // vec1 32 ssa_350 = phi block_8: ssa_107, block_15: ssa_338
		mov.b32 %ssa_351, %ssa_106; // vec1 32 ssa_351 = phi block_8: ssa_106, block_15: ssa_339
			// succs: block_16 
			// end_block block_8:
			bra end_if_2;
		
		else_2: 
			// start_block block_9:
			// preds: block_7 
			.reg .pred %ssa_252;
			setp.eq.s32 %ssa_252, %ssa_142, %ssa_1_bits; // vec1 1 ssa_252 = ieq ssa_142, ssa_1

			// succs: block_10 block_11 
			// end_block block_9:
			//if
			@!%ssa_252 bra else_3;
			
				// start_block block_10:
				// preds: block_9 
				.reg .b64 %ssa_253;
	add.u64 %ssa_253, %ssa_139, 0; // vec1 32 ssa_253 = deref_struct &ssa_139->field0 (function_temp vec4) /* &hitValue.field0 */

				.reg .f32 %ssa_254_0;
				.reg .f32 %ssa_254_1;
				.reg .f32 %ssa_254_2;
				.reg .f32 %ssa_254_3;
				ld.global.f32 %ssa_254_0, [%ssa_253 + 0];
				ld.global.f32 %ssa_254_1, [%ssa_253 + 4];
				ld.global.f32 %ssa_254_2, [%ssa_253 + 8];
				ld.global.f32 %ssa_254_3, [%ssa_253 + 12];
// vec4 32 ssa_254 = intrinsic load_deref (%ssa_253) (0) /* access=0 */


				.reg .f32 %ssa_255;
				rcp.approx.f32 %ssa_255, %ssa_254_0; // vec1 32 ssa_255 = frcp ssa_254.x

				.reg .f32 %ssa_256;
				mul.f32 %ssa_256, %ssa_144_2, %ssa_110; // vec1 32 ssa_256 = fmul ssa_144.z, ssa_110

				.reg .f32 %ssa_257;
				mul.f32 %ssa_257, %ssa_144_1, %ssa_111; // vec1 32 ssa_257 = fmul ssa_144.y, ssa_111

				.reg .f32 %ssa_258;
				add.f32 %ssa_258, %ssa_256, %ssa_257;	// vec1 32 ssa_258 = fadd ssa_256, ssa_257

				.reg .f32 %ssa_259;
				mul.f32 %ssa_259, %ssa_144_0, %ssa_112; // vec1 32 ssa_259 = fmul ssa_144.x, ssa_112

				.reg .f32 %ssa_260;
				add.f32 %ssa_260, %ssa_258, %ssa_259;	// vec1 32 ssa_260 = fadd ssa_258, ssa_259

				.reg .f32 %ssa_261;
				abs.f32 %ssa_261, %ssa_260;	// vec1 32 ssa_261 = fabs ssa_260

				.reg .f32 %ssa_262;
				add.f32 %ssa_262, %ssa_254_0, %ssa_34; // vec1 32 ssa_262 = fadd ssa_254.x, ssa_34

				.reg .f32 %ssa_263;
	mov.f32 %ssa_263, 0F42c80000; // vec1 32 ssa_263 = load_const (0x42c80000 /* 100.000000 */)
				.reg .b32 %ssa_263_bits;
	mov.f32 %ssa_263_bits, 0F42c80000;

				.reg .f32 %ssa_264;
				mul.f32 %ssa_264, %ssa_262, %ssa_263;	// vec1 32 ssa_264 = fmul ssa_262, ssa_263

				.reg .f32 %ssa_265;
				mul.f32 %ssa_265, %ssa_112, %ssa_255;	// vec1 32 ssa_265 = fmul ssa_112, ssa_255

				.reg .f32 %ssa_266;
				mul.f32 %ssa_266, %ssa_111, %ssa_255;	// vec1 32 ssa_266 = fmul ssa_111, ssa_255

				.reg .f32 %ssa_267;
				mul.f32 %ssa_267, %ssa_110, %ssa_255;	// vec1 32 ssa_267 = fmul ssa_110, ssa_255

				.reg .f32 %ssa_268;
				mul.f32 %ssa_268, %ssa_109, %ssa_255;	// vec1 32 ssa_268 = fmul ssa_109, ssa_255

				.reg .f32 %ssa_269;
				mul.f32 %ssa_269, %ssa_255, %ssa_261;	// vec1 32 ssa_269 = fmul ssa_255, ssa_261

				.reg .f32 %ssa_270;
				mul.f32 %ssa_270, %ssa_255, %ssa_255;	// vec1 32 ssa_270 = fmul ssa_255, ssa_255

				.reg .f32 %ssa_271;
				mul.f32 %ssa_271, %ssa_261, %ssa_261;	// vec1 32 ssa_271 = fmul ssa_261, ssa_261

				.reg .f32 %ssa_272;
				neg.f32 %ssa_272, %ssa_271;	// vec1 32 ssa_272 = fneg ssa_271

				.reg .f32 %ssa_273;
				add.f32 %ssa_273, %ssa_3, %ssa_272;	// vec1 32 ssa_273 = fadd ssa_3, ssa_272

				.reg .f32 %ssa_274;
				mul.f32 %ssa_274, %ssa_270, %ssa_273;	// vec1 32 ssa_274 = fmul ssa_270, ssa_273

				.reg .f32 %ssa_275;
				neg.f32 %ssa_275, %ssa_274;	// vec1 32 ssa_275 = fneg ssa_274

				.reg .f32 %ssa_276;
				add.f32 %ssa_276, %ssa_3, %ssa_275;	// vec1 32 ssa_276 = fadd ssa_3, ssa_275

				.reg .f32 %ssa_277;
				neg.f32 %ssa_277, %ssa_276;	// vec1 32 ssa_277 = fneg ssa_276

				.reg .f32 %ssa_278;
				add.f32 %ssa_278, %ssa_269, %ssa_277;	// vec1 32 ssa_278 = fadd ssa_269, ssa_277

				.reg .f32 %ssa_279;
				add.f32 %ssa_279, %ssa_265, %ssa_278;	// vec1 32 ssa_279 = fadd ssa_265, ssa_278

				.reg .f32 %ssa_280;
				add.f32 %ssa_280, %ssa_266, %ssa_278;	// vec1 32 ssa_280 = fadd ssa_266, ssa_278

				.reg .f32 %ssa_281;
				add.f32 %ssa_281, %ssa_267, %ssa_278;	// vec1 32 ssa_281 = fadd ssa_267, ssa_278

				.reg .f32 %ssa_282;
				add.f32 %ssa_282, %ssa_268, %ssa_278;	// vec1 32 ssa_282 = fadd ssa_268, ssa_278

				.reg .f32 %ssa_283;
				sub.f32 %ssa_283, %const1_f32, %ssa_264;
				mul.f32 %ssa_283, %ssa_112, %ssa_283;
				mul.f32 %temp_f32, %ssa_264, %ssa_279;
				add.f32 %ssa_283, %ssa_283, %temp_f32; // vec1 32 ssa_283 = flrp ssa_112, ssa_279, ssa_264

				.reg .f32 %ssa_284;
				sub.f32 %ssa_284, %const1_f32, %ssa_264;
				mul.f32 %ssa_284, %ssa_111, %ssa_284;
				mul.f32 %temp_f32, %ssa_264, %ssa_280;
				add.f32 %ssa_284, %ssa_284, %temp_f32; // vec1 32 ssa_284 = flrp ssa_111, ssa_280, ssa_264

				.reg .f32 %ssa_285;
				sub.f32 %ssa_285, %const1_f32, %ssa_264;
				mul.f32 %ssa_285, %ssa_110, %ssa_285;
				mul.f32 %temp_f32, %ssa_264, %ssa_281;
				add.f32 %ssa_285, %ssa_285, %temp_f32; // vec1 32 ssa_285 = flrp ssa_110, ssa_281, ssa_264

				.reg .f32 %ssa_286;
				sub.f32 %ssa_286, %const1_f32, %ssa_264;
				mul.f32 %ssa_286, %ssa_109, %ssa_286;
				mul.f32 %temp_f32, %ssa_264, %ssa_282;
				add.f32 %ssa_286, %ssa_286, %temp_f32; // vec1 32 ssa_286 = flrp ssa_109, ssa_282, ssa_264

				.reg .f32 %ssa_287;
				mul.f32 %ssa_287, %ssa_286, %ssa_286;	// vec1 32 ssa_287 = fmul ssa_286, ssa_286

				.reg .f32 %ssa_288;
				mul.f32 %ssa_288, %ssa_285, %ssa_285;	// vec1 32 ssa_288 = fmul ssa_285, ssa_285

				.reg .f32 %ssa_289;
				add.f32 %ssa_289, %ssa_287, %ssa_288;	// vec1 32 ssa_289 = fadd ssa_287, ssa_288

				.reg .f32 %ssa_290;
				mul.f32 %ssa_290, %ssa_284, %ssa_284;	// vec1 32 ssa_290 = fmul ssa_284, ssa_284

				.reg .f32 %ssa_291;
				add.f32 %ssa_291, %ssa_289, %ssa_290;	// vec1 32 ssa_291 = fadd ssa_289, ssa_290

				.reg .f32 %ssa_292;
				mul.f32 %ssa_292, %ssa_283, %ssa_283;	// vec1 32 ssa_292 = fmul ssa_283, ssa_283

				.reg .f32 %ssa_293;
				add.f32 %ssa_293, %ssa_291, %ssa_292;	// vec1 32 ssa_293 = fadd ssa_291, ssa_292

				.reg .f32 %ssa_294;
				rsqrt.approx.f32 %ssa_294, %ssa_293;	// vec1 32 ssa_294 = frsq ssa_293

				.reg .f32 %ssa_295;
				mul.f32 %ssa_295, %ssa_283, %ssa_294;	// vec1 32 ssa_295 = fmul ssa_283, ssa_294

				.reg .f32 %ssa_296;
				mul.f32 %ssa_296, %ssa_284, %ssa_294;	// vec1 32 ssa_296 = fmul ssa_284, ssa_294

				.reg .f32 %ssa_297;
				mul.f32 %ssa_297, %ssa_285, %ssa_294;	// vec1 32 ssa_297 = fmul ssa_285, ssa_294

				.reg .f32 %ssa_298;
				mul.f32 %ssa_298, %ssa_286, %ssa_294;	// vec1 32 ssa_298 = fmul ssa_286, ssa_294

				.reg .f32 %ssa_299;
				mov.f32 %ssa_299, %ssa_141_0; // vec1 32 ssa_299 = mov ssa_141.x

				.reg .f32 %ssa_300;
				mov.f32 %ssa_300, %ssa_141_1; // vec1 32 ssa_300 = mov ssa_141.y

				.reg .f32 %ssa_301;
				mov.f32 %ssa_301, %ssa_141_2; // vec1 32 ssa_301 = mov ssa_141.z

		mov.b32 %ssa_329, %ssa_116; // vec1 32 ssa_329 = phi block_10: ssa_116, block_14: ssa_322
		mov.b32 %ssa_330, %ssa_115; // vec1 32 ssa_330 = phi block_10: ssa_115, block_14: ssa_323
		mov.b32 %ssa_331, %ssa_114; // vec1 32 ssa_331 = phi block_10: ssa_114, block_14: ssa_324
		mov.b32 %ssa_332, %ssa_113; // vec1 32 ssa_332 = phi block_10: ssa_113, block_14: ssa_325
				mov.b32 %ssa_333, %ssa_295; // vec1 32 ssa_333 = phi block_10: ssa_295, block_14: ssa_112
				mov.b32 %ssa_334, %ssa_296; // vec1 32 ssa_334 = phi block_10: ssa_296, block_14: ssa_111
				mov.b32 %ssa_335, %ssa_297; // vec1 32 ssa_335 = phi block_10: ssa_297, block_14: ssa_110
				mov.b32 %ssa_336, %ssa_298; // vec1 32 ssa_336 = phi block_10: ssa_298, block_14: ssa_109
				mov.b32 %ssa_337, %ssa_299; // vec1 32 ssa_337 = phi block_10: ssa_299, block_14: ssa_326
				mov.b32 %ssa_338, %ssa_300; // vec1 32 ssa_338 = phi block_10: ssa_300, block_14: ssa_327
				mov.b32 %ssa_339, %ssa_301; // vec1 32 ssa_339 = phi block_10: ssa_301, block_14: ssa_328
				// succs: block_15 
				// end_block block_10:
				bra end_if_3;
			
			else_3: 
				// start_block block_11:
				// preds: block_9 
				.reg .pred %ssa_302;
				setp.eq.s32 %ssa_302, %ssa_142, %ssa_4_bits; // vec1 1 ssa_302 = ieq ssa_142, ssa_4

				// succs: block_12 block_13 
				// end_block block_11:
				//if
				@!%ssa_302 bra else_4;
				
					// start_block block_12:
					// preds: block_11 
					.reg .b64 %ssa_303;
	add.u64 %ssa_303, %ssa_139, 0; // vec1 32 ssa_303 = deref_struct &ssa_139->field0 (function_temp vec4) /* &hitValue.field0 */

					.reg .f32 %ssa_304_0;
					.reg .f32 %ssa_304_1;
					.reg .f32 %ssa_304_2;
					.reg .f32 %ssa_304_3;
					ld.global.f32 %ssa_304_0, [%ssa_303 + 0];
					ld.global.f32 %ssa_304_1, [%ssa_303 + 4];
					ld.global.f32 %ssa_304_2, [%ssa_303 + 8];
					ld.global.f32 %ssa_304_3, [%ssa_303 + 12];
// vec4 32 ssa_304 = intrinsic load_deref (%ssa_303) (0) /* access=0 */


					.reg .f32 %ssa_305;
					neg.f32 %ssa_305, %ssa_113;	// vec1 32 ssa_305 = fneg ssa_113

					.reg .f32 %ssa_306;
					add.f32 %ssa_306, %ssa_3, %ssa_305;	// vec1 32 ssa_306 = fadd ssa_3, ssa_305

					.reg .f32 %ssa_307;
					mul.f32 %ssa_307, %ssa_304_0, %ssa_306; // vec1 32 ssa_307 = fmul ssa_304.x, ssa_306

					.reg .f32 %ssa_308;
					mul.f32 %ssa_308, %ssa_304_1, %ssa_306; // vec1 32 ssa_308 = fmul ssa_304.y, ssa_306

					.reg .f32 %ssa_309;
					mul.f32 %ssa_309, %ssa_304_2, %ssa_306; // vec1 32 ssa_309 = fmul ssa_304.z, ssa_306

					.reg .f32 %ssa_310;
					mul.f32 %ssa_310, %ssa_307, %ssa_304_3; // vec1 32 ssa_310 = fmul ssa_307, ssa_304.w

					.reg .f32 %ssa_311;
					mul.f32 %ssa_311, %ssa_308, %ssa_304_3; // vec1 32 ssa_311 = fmul ssa_308, ssa_304.w

					.reg .f32 %ssa_312;
					mul.f32 %ssa_312, %ssa_309, %ssa_304_3; // vec1 32 ssa_312 = fmul ssa_309, ssa_304.w

					.reg .f32 %ssa_313;
					add.f32 %ssa_313, %ssa_116, %ssa_310;	// vec1 32 ssa_313 = fadd ssa_116, ssa_310

					.reg .f32 %ssa_314;
					add.f32 %ssa_314, %ssa_115, %ssa_311;	// vec1 32 ssa_314 = fadd ssa_115, ssa_311

					.reg .f32 %ssa_315;
					add.f32 %ssa_315, %ssa_114, %ssa_312;	// vec1 32 ssa_315 = fadd ssa_114, ssa_312

					.reg .f32 %ssa_316;
					mul.f32 %ssa_316, %ssa_2, %ssa_306;	// vec1 32 ssa_316 = fmul ssa_2, ssa_306

					.reg .f32 %ssa_317;
					mul.f32 %ssa_317, %ssa_316, %ssa_304_3; // vec1 32 ssa_317 = fmul ssa_316, ssa_304.w

					.reg .f32 %ssa_318;
					add.f32 %ssa_318, %ssa_113, %ssa_317;	// vec1 32 ssa_318 = fadd ssa_113, ssa_317

					.reg .f32 %ssa_319;
					mov.f32 %ssa_319, %ssa_141_0; // vec1 32 ssa_319 = mov ssa_141.x

					.reg .f32 %ssa_320;
					mov.f32 %ssa_320, %ssa_141_1; // vec1 32 ssa_320 = mov ssa_141.y

					.reg .f32 %ssa_321;
					mov.f32 %ssa_321, %ssa_141_2; // vec1 32 ssa_321 = mov ssa_141.z

					mov.b32 %ssa_322, %ssa_313; // vec1 32 ssa_322 = phi block_12: ssa_313, block_13: ssa_116
					mov.b32 %ssa_323, %ssa_314; // vec1 32 ssa_323 = phi block_12: ssa_314, block_13: ssa_115
					mov.b32 %ssa_324, %ssa_315; // vec1 32 ssa_324 = phi block_12: ssa_315, block_13: ssa_114
					mov.b32 %ssa_325, %ssa_318; // vec1 32 ssa_325 = phi block_12: ssa_318, block_13: ssa_113
					mov.b32 %ssa_326, %ssa_319; // vec1 32 ssa_326 = phi block_12: ssa_319, block_13: ssa_108
					mov.b32 %ssa_327, %ssa_320; // vec1 32 ssa_327 = phi block_12: ssa_320, block_13: ssa_107
					mov.b32 %ssa_328, %ssa_321; // vec1 32 ssa_328 = phi block_12: ssa_321, block_13: ssa_106
					// succs: block_14 
					// end_block block_12:
					bra end_if_4;
				
				else_4: 
					// start_block block_13:
					// preds: block_11 
		mov.b32 %ssa_322, %ssa_116; // vec1 32 ssa_322 = phi block_12: ssa_313, block_13: ssa_116
		mov.b32 %ssa_323, %ssa_115; // vec1 32 ssa_323 = phi block_12: ssa_314, block_13: ssa_115
		mov.b32 %ssa_324, %ssa_114; // vec1 32 ssa_324 = phi block_12: ssa_315, block_13: ssa_114
		mov.b32 %ssa_325, %ssa_113; // vec1 32 ssa_325 = phi block_12: ssa_318, block_13: ssa_113
		mov.b32 %ssa_326, %ssa_108; // vec1 32 ssa_326 = phi block_12: ssa_319, block_13: ssa_108
		mov.b32 %ssa_327, %ssa_107; // vec1 32 ssa_327 = phi block_12: ssa_320, block_13: ssa_107
		mov.b32 %ssa_328, %ssa_106; // vec1 32 ssa_328 = phi block_12: ssa_321, block_13: ssa_106
					// succs: block_14 
					// end_block block_13:
				end_if_4:
				// start_block block_14:
				// preds: block_12 block_13 







				mov.b32 %ssa_329, %ssa_322; // vec1 32 ssa_329 = phi block_10: ssa_116, block_14: ssa_322
				mov.b32 %ssa_330, %ssa_323; // vec1 32 ssa_330 = phi block_10: ssa_115, block_14: ssa_323
				mov.b32 %ssa_331, %ssa_324; // vec1 32 ssa_331 = phi block_10: ssa_114, block_14: ssa_324
				mov.b32 %ssa_332, %ssa_325; // vec1 32 ssa_332 = phi block_10: ssa_113, block_14: ssa_325
		mov.b32 %ssa_333, %ssa_112; // vec1 32 ssa_333 = phi block_10: ssa_295, block_14: ssa_112
		mov.b32 %ssa_334, %ssa_111; // vec1 32 ssa_334 = phi block_10: ssa_296, block_14: ssa_111
		mov.b32 %ssa_335, %ssa_110; // vec1 32 ssa_335 = phi block_10: ssa_297, block_14: ssa_110
		mov.b32 %ssa_336, %ssa_109; // vec1 32 ssa_336 = phi block_10: ssa_298, block_14: ssa_109
				mov.b32 %ssa_337, %ssa_326; // vec1 32 ssa_337 = phi block_10: ssa_299, block_14: ssa_326
				mov.b32 %ssa_338, %ssa_327; // vec1 32 ssa_338 = phi block_10: ssa_300, block_14: ssa_327
				mov.b32 %ssa_339, %ssa_328; // vec1 32 ssa_339 = phi block_10: ssa_301, block_14: ssa_328
				// succs: block_15 
				// end_block block_14:
			end_if_3:
			// start_block block_15:
			// preds: block_10 block_14 











		mov.b32 %ssa_340, %ssa_117; // vec1 32 ssa_340 = phi block_8: ssa_10, block_15: ssa_117
			mov.b32 %ssa_341, %ssa_329; // vec1 32 ssa_341 = phi block_8: ssa_248, block_15: ssa_329
			mov.b32 %ssa_342, %ssa_330; // vec1 32 ssa_342 = phi block_8: ssa_249, block_15: ssa_330
			mov.b32 %ssa_343, %ssa_331; // vec1 32 ssa_343 = phi block_8: ssa_250, block_15: ssa_331
			mov.b32 %ssa_344, %ssa_332; // vec1 32 ssa_344 = phi block_8: ssa_251, block_15: ssa_332
			mov.b32 %ssa_345, %ssa_333; // vec1 32 ssa_345 = phi block_8: ssa_112, block_15: ssa_333
			mov.b32 %ssa_346, %ssa_334; // vec1 32 ssa_346 = phi block_8: ssa_111, block_15: ssa_334
			mov.b32 %ssa_347, %ssa_335; // vec1 32 ssa_347 = phi block_8: ssa_110, block_15: ssa_335
			mov.b32 %ssa_348, %ssa_336; // vec1 32 ssa_348 = phi block_8: ssa_109, block_15: ssa_336
			mov.b32 %ssa_349, %ssa_337; // vec1 32 ssa_349 = phi block_8: ssa_108, block_15: ssa_337
			mov.b32 %ssa_350, %ssa_338; // vec1 32 ssa_350 = phi block_8: ssa_107, block_15: ssa_338
			mov.b32 %ssa_351, %ssa_339; // vec1 32 ssa_351 = phi block_8: ssa_106, block_15: ssa_339
			// succs: block_16 
			// end_block block_15:
		end_if_2:
		// start_block block_16:
		// preds: block_8 block_15 












		.reg .s32 %ssa_352;
		add.s32 %ssa_352, %ssa_118, %ssa_1_bits; // vec1 32 ssa_352 = iadd ssa_118, ssa_1

		.reg .f32 %ssa_353;
		min.f32 %ssa_353, %ssa_341, %ssa_343;	// vec1 32 ssa_353 = fmin! ssa_341, ssa_343

		.reg .f32 %ssa_354;
		min.f32 %ssa_354, %ssa_353, %ssa_342;	// vec1 32 ssa_354 = fmin! ssa_353, ssa_342

		mov.b32 %ssa_106, %ssa_351; // vec1 32 ssa_106 = phi block_0: ssa_105, block_16: ssa_351
		mov.b32 %ssa_107, %ssa_350; // vec1 32 ssa_107 = phi block_0: ssa_104, block_16: ssa_350
		mov.b32 %ssa_108, %ssa_349; // vec1 32 ssa_108 = phi block_0: ssa_103, block_16: ssa_349
		mov.b32 %ssa_109, %ssa_348; // vec1 32 ssa_109 = phi block_0: ssa_102, block_16: ssa_348
		mov.b32 %ssa_110, %ssa_347; // vec1 32 ssa_110 = phi block_0: ssa_101, block_16: ssa_347
		mov.b32 %ssa_111, %ssa_346; // vec1 32 ssa_111 = phi block_0: ssa_100, block_16: ssa_346
		mov.b32 %ssa_112, %ssa_345; // vec1 32 ssa_112 = phi block_0: ssa_99, block_16: ssa_345
		mov.b32 %ssa_113, %ssa_344; // vec1 32 ssa_113 = phi block_0: ssa_5, block_16: ssa_344
		mov.b32 %ssa_114, %ssa_343; // vec1 32 ssa_114 = phi block_0: ssa_5, block_16: ssa_343
		mov.b32 %ssa_115, %ssa_342; // vec1 32 ssa_115 = phi block_0: ssa_5, block_16: ssa_342
		mov.b32 %ssa_116, %ssa_341; // vec1 32 ssa_116 = phi block_0: ssa_5, block_16: ssa_341
		mov.b32 %ssa_117, %ssa_340; // vec1 32 ssa_117 = phi block_0: ssa_5, block_16: ssa_340
		mov.s32 %ssa_118, %ssa_352; // vec1 32 ssa_118 = phi block_0: ssa_5, block_16: ssa_352
		mov.f32 %ssa_119, %ssa_354; // vec1 32 ssa_119 = phi block_0: ssa_5, block_16: ssa_354
		// succs: block_1 
		// end_block block_16:
		bra loop_0;
	
	loop_0_exit:
	// start_block block_17:
	// preds: block_3 
	.reg .b32 %ssa_355_0;
	.reg .b32 %ssa_355_1;
	.reg .b32 %ssa_355_2;
	.reg .b32 %ssa_355_3;
	mov.b32 %ssa_355_0, %ssa_116;
	mov.b32 %ssa_355_1, %ssa_115;
	mov.b32 %ssa_355_2, %ssa_114;
	mov.b32 %ssa_355_3, %ssa_113; // vec4 32 ssa_355 = vec4 ssa_116, ssa_115, ssa_114, ssa_113

	.reg .b64 %ssa_356;
	mov.b64 %ssa_356, %image; // vec1 32 ssa_356 = deref_var &image (uniform image2D) 

	.reg .u32 %ssa_357_0;
	.reg .u32 %ssa_357_1;
	.reg .u32 %ssa_357_2;
	.reg .u32 %ssa_357_3;
	mov.u32 %ssa_357_0, %ssa_20_0;
	mov.u32 %ssa_357_1, %ssa_20_1;
	mov.u32 %ssa_357_2, %ssa_20_1;
	mov.u32 %ssa_357_3, %ssa_20_1; // vec4 32 ssa_357 = vec4 ssa_20.x, ssa_20.y, ssa_20.y, ssa_20.y

	image_deref_store %ssa_356, %ssa_357_0, %ssa_357_1, %ssa_357_2, %ssa_357_3, %ssa_0, %ssa_355_0, %ssa_355_1, %ssa_355_2, %ssa_355_3, %ssa_5, 0, 160; // intrinsic image_deref_store (%ssa_356, %ssa_357, %ssa_0, %ssa_355, %ssa_5) (0, 160) /* access=0 */ /* src_type=float32 */

	// succs: block_18 
	// end_block block_17:
	// block block_18:
	shader_exit:
	ret ;
}
